{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2: Word representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will implement the **skip-gram model with negative sampling (SGNS)** from Lecture&nbsp;2.4, and use it to train word embeddings on the text of the [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page).\n",
    "\n",
    "‚ö†Ô∏è The dataset for this lab contains 18M tokens. This is very little as far as word embedding datasets are concerned ‚Äì for example, the original word2vec model was pre-trained on 100B tokens. In spite of this, you will need to think about efficiency when processing the data and training your models. In particular, wherever possible you should use iterators rather than lists, and vectorize operations (using [NumPy](https://numpy.org) or [PyTorch](https://pytorch.org)) as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this lab comes as a bz2-compressed plain text file. It consists of 1,163,769 sentences, with one sentence per line and tokens separated by spaces. The cell below contains a wrapper class `SimpleWikiDataset` that can be used to iterate over the sentences (lines) in the text file. On the Python side of things, each sentence is represented as a list of tokens (strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "\n",
    "class SimpleWikiDataset():\n",
    "    \n",
    "    def __init__(self, max_sentences=None):\n",
    "        self.max_sentences = max_sentences\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with bz2.open('simplewiki.txt.bz2', 'rt', encoding='utf-8') as sentences:\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                if self.max_sentences and i >= self.max_sentences:\n",
    "                    break\n",
    "                yield sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this class, we define two variants of the dataset: the full dataset and a minimal version with the first 1% of the sentences in the full dataset. The latter will be useful to test code without running it on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with all sentences (N = 1,163,769)\n",
    "full_dataset = SimpleWikiDataset()\n",
    "\n",
    "# Minimal dataset\n",
    "mini_dataset = SimpleWikiDataset(max_sentences=11638)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell defines a generator function that allows you to iterate over all tokens in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(sentences):\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how to use this function, here is code that prints the number of tokens in the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17594885\n"
     ]
    }
   ],
   "source": [
    "print(sum(1 for t in tokens(full_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Build the vocabulary and frequency table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to construct the embedding **vocabulary** ‚Äì the set of unique words that will receive an embedding. Because you will eventually need to map words to vector dimensions, you will represent the vocabulary as a dictionary that maps words (strings) to a contiguous range of integers.\n",
    "\n",
    "Along with the vocabulary, you will also construct the **frequency table**, that is, the table that holds the absolute frequencies (counts) in the data, for all words in your vocabulary. This will simply be an array of integers, indexed by the word ids in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the vocabulary and the frequency table, complete the skeleton code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_vocab_and_counts(sentences, min_count=5):\n",
    "    word_freq = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word in word_freq:\n",
    "                word_freq[word] += 1\n",
    "            else:\n",
    "                word_freq[word] = 1\n",
    "    \n",
    "    filtered_words = {word: count for word, count in word_freq.items() if count >= min_count}\n",
    "    \n",
    "    vocab = {word: idx for idx, word in enumerate(filtered_words.keys())}\n",
    "    \n",
    "    counts = np.array(list(filtered_words.values()), dtype=np.int32)\n",
    "    \n",
    "    return vocab, counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should comply with the following specification:\n",
    "\n",
    "**make_vocab_and_counts** (*sentences*, *min_count* = 5)\n",
    "\n",
    "> Reads from an iterable of *sentences* (lists of string tokens) and returns a pair *vocab*, *counts* where *vocab* is a dictionary representing the vocabulary and *counts* is a 1D-[ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) with the absolute frequencies (counts) of the words in the vocabulary. The dictionary *vocab* maps words to a contiguous range of integers starting at&nbsp;0. In the *counts* array, the entry at index $i$ is the count of that word in *vocab* which maps to $i$. Words that occur less than *min_count* times are excluded from the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§û Test your code\n",
    "\n",
    "To test your code, print the sizes of the vocabularies constructed from the two datasets, as well as the count totals. The correct vocabulary size for the minimal dataset is 3,231; for the full dataset, the correct vocabulary size is 73,339. The correct totals are 155,818 for the minimal dataset and 17,297,355 for the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal Dataset: Vocabulary Size = 3231, Total Counts = 155818\n",
      "Full Dataset: Vocabulary Size = 73339, Total Counts = 17297355\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "vocab_minimal, counts_minimal = make_vocab_and_counts(mini_dataset)\n",
    "vocab_full, counts_full = make_vocab_and_counts(full_dataset)\n",
    "\n",
    "vocab_size_minimal = len(vocab_minimal)\n",
    "vocab_size_full = len(vocab_full)\n",
    "\n",
    "\n",
    "vocab_minimal_all, counts_minimal_all = make_vocab_and_counts(mini_dataset,-1)\n",
    "total_counts_minimal_all  = counts_minimal_all.sum()\n",
    "\n",
    "vocab_full_all, counts_full_all = make_vocab_and_counts(full_dataset,-1)\n",
    "total_counts_full_all = counts_full_all.sum()\n",
    "\n",
    "total_counts_minimal = counts_minimal.sum()\n",
    "total_counts_full = counts_full.sum()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Minimal Dataset: Vocabulary Size = {vocab_size_minimal}, Total Counts = {total_counts_minimal}\")\n",
    "print(f\"Full Dataset: Vocabulary Size = {vocab_size_full}, Total Counts = {total_counts_full}\")\n",
    "\n",
    "assert vocab_size_minimal == 3231, f\"Unexpected minimal dataset vocabulary size: {vocab_size_minimal}\"\n",
    "assert vocab_size_full == 73339, f\"Unexpected full dataset vocabulary size: {vocab_size_full}\"\n",
    "assert total_counts_minimal == 155818, f\"Unexpected total counts for minimal dataset: {total_counts_minimal}\"\n",
    "assert total_counts_full == 17297355, f\"Unexpected total counts for full dataset: {total_counts_full}\"\n",
    "\n",
    "print(\"All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to preprocess the training data. This involves the following:\n",
    "\n",
    "* Discard words that are not in the vocabulary\n",
    "* Map each word to its vocabulary id\n",
    "* Randomly discard words according to the subsampling strategy covered in Lecture&nbsp;2.4\n",
    "* Discard sentences that have become empty\n",
    "\n",
    "As a reminder, the subsampling strategy involves discarding tokens $w$ with probability\n",
    "\n",
    "$$\n",
    "P(w) = \\max (0, 1-\\sqrt{tN/\\#(w)})\n",
    "$$\n",
    "\n",
    "where $\\#(w)$ is the count of $w$, $N$ is the total number of counts, and $t$ is the chosen threshold (default value: 0.001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains skeleton code for a generator function `preprocess`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def preprocess(vocab, counts, sentences, threshold=0.001):\n",
    "    N = np.sum(counts)  # Total count of all words\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        preprocessed_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in vocab:  \n",
    "                word_id = vocab[word]  \n",
    "                \n",
    "\n",
    "                word_count = counts[word_id]\n",
    "                P_w = max(0, 1 - np.sqrt(threshold * N / word_count))\n",
    "                if random.random() > P_w: \n",
    "                    preprocessed_sentence.append(word_id)\n",
    "        \n",
    "        if preprocessed_sentence:\n",
    "            yield preprocessed_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend this skeleton code into a function that implements the preprocessing. Your code should comply with the following specification:\n",
    "\n",
    "**preprocess** (*vocab*, *counts*, *sentences*, *threshold* = 0.001)\n",
    "\n",
    "> Reads from an iterable of *sentences* (lists of string tokens) and yields the preprocessed sentences as non-empty lists of word ids (integers). Words not in *vocab* are discarded. The remaining words are randomly discarded according to the subsampling strategy with the given *threshold*. In the non-empty sentences, each token is replaced by its id in the vocabulary.\n",
    "\n",
    "**‚ö†Ô∏è Please observe** that your function should *yield* the preprocessed sentences, not return a list with all of them. That is, we ask you to write a *generator function*. If you have not worked with generators and iterators before, now is a good time to read up on them. [More information about generators](https://wiki.python.org/moin/Generators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§û Test your code\n",
    "\n",
    "Test your code by comparing the total number of tokens in the preprocessed version of each dataset with the corresponding number for the original data. The former should be ca. 59% of the latter for the minimal dataset, and ca. 69% for the full dataset. The exact percentage will vary slightly because of the randomness in the sampling. You may want to repeat your computation several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal Dataset: 59.24% tokens retained after preprocessing.\n",
      "Full Dataset: 69.46% tokens retained after preprocessing.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def compute_total_tokens(sentences):\n",
    "    return sum(len(sentence) for sentence in sentences)\n",
    "\n",
    "total_tokens_minimal_original = compute_total_tokens(mini_dataset)\n",
    "total_tokens_full_original = compute_total_tokens(full_dataset)\n",
    "\n",
    "preprocessed_sentences_minimal = list(preprocess(vocab_minimal, counts_minimal, mini_dataset))\n",
    "preprocessed_sentences_full = list(preprocess(vocab_full, counts_full, full_dataset))\n",
    "\n",
    "\n",
    "total_tokens_minimal_preprocessed = compute_total_tokens(preprocessed_sentences_minimal)\n",
    "total_tokens_full_preprocessed = compute_total_tokens(preprocessed_sentences_full)\n",
    "\n",
    "\n",
    "percentage_minimal = (total_tokens_minimal_preprocessed / total_tokens_minimal_original) * 100\n",
    "percentage_full = (total_tokens_full_preprocessed / total_tokens_full_original) * 100\n",
    "\n",
    "print(f\"Minimal Dataset: {percentage_minimal:.2f}% tokens retained after preprocessing.\")\n",
    "print(f\"Full Dataset: {percentage_full:.2f}% tokens retained after preprocessing.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Generate the training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to translate the preprocessed sentences into training examples for the skip-gram model: both *positive examples* (target word‚Äìcontext word pairs actually observed in the data) and *negative examples* (pairs randomly sampled from a noise distribution).\n",
    "\n",
    "**‚ö†Ô∏è We expect that solving this problem will take you the longest time in this lab.**\n",
    "\n",
    "### General strategy\n",
    "\n",
    "The general plan for solving this problem is to implement a generator function that traverses the preprocessed sentences, at each position of the text samples a window, and then extracts all positive examples from it. For each positive example, the function also generates $k$ negative examples, where $k$ is a hyperparameter. Finally, all examples (positive and negative) are combined into the tensor representation described below.\n",
    "\n",
    "### Representation\n",
    "\n",
    "How should you represent a batch of training examples? Writing $B$ for the batch size, the obvious choice would be to represent the inputs as a matrix of shape $[B, 2]$ and the output labels (positive/negative) as a vector of length $B$. This representation would be quite wasteful on the input side, however, as each target word (index) from a positive example would have to be repeated in all negative samples. For example ($k=3$):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensor([[34,  237],    # positive example 1\n",
    "        [34, 2561],    # negative example 1.1\n",
    "        [34,   39],    # negative example 1.2\n",
    "        [34,  903],    # negative example 1.3\n",
    "        [34, 2036],    # positive example 2\n",
    "        [34, 2132],    # negative example 2.1\n",
    "        [34,  576],    # negative example 2.2\n",
    "        [34, 2437]])   # negative example 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will use a different representation: First, instead of a single input batch, there will be a *pair* of input batches ‚Äì a vector for the target words and a matrix for the context words. If the target word vector has length $B$, the context word matrix has shape $[B, 1+k]$. The $i$th element of the target word vector is the target word for *all* context words in the $i$th row of the context word matrix: the first column of that row comes from a positive example, the remaining columns come from the $k$ negative samples. Accordingly, the batch with the output labels will be a matrix of the same shape as the context word matrix, with its first column set to&nbsp;1 and its remaining columns set to&nbsp;0. Corresponding to the example above:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# input batch component 1: target word vector\n",
    "tensor([34, 34])\n",
    "\n",
    "# input batch component 2: context word matrix\n",
    "tensor([[237, 2561, 39, 903], [2036, 2132, 576, 2437]])\n",
    "\n",
    "# output labels\n",
    "tensor([[1, 0, 0, 0], [1, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the present problem, you will only be concerned with the two input batches; the output batch will be constructed in the training procedure. In fact, for a fixed batch size $B$, that batch is always exactly the same, so you will only have to build it once.\n",
    "\n",
    "### Negative sampling\n",
    "\n",
    "Recall from Lecture&nbsp;2.4 that the probability of a word $c$ to be selected as the context word in a negative sample is proportional to its exponentiated count $\\#(c)^\\alpha$, where $\\alpha$ is a hyperparameter (default value: 0.75).\n",
    "\n",
    "To implement negative sampling from this distribution, you can follow a standard recipe: Start by pre-computing an array containing the *cumulative sums* of the exponentiated counts. Then, generate a random cumulative count $n$, and find that index in the pre-computed array at which $n$ should be inserted to keep the array sorted. That index identifies the sampled context word.\n",
    "\n",
    "All operations in this recipe can be implemented efficiently in PyTorch; the relevant functions are [`torch.cumsum`](https://pytorch.org/docs/stable/generated/torch.cumsum.html) and [`torch.searchsorted`](https://pytorch.org/docs/stable/generated/torch.searchsorted.html). For optimal efficiency, you should sample all $B \\times k$ negative examples in a batch at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is skeleton code for this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def training_examples(vocab, counts, sentences, window=5, num_ns=5, batch_size=1<<19, ns_exponent=0.75):\n",
    "    # doesnt use vocab because the preprocess was done outside this func\n",
    "    freqs = np.array(counts)**ns_exponent\n",
    "    norm_freqs = freqs / freqs.sum()\n",
    "    cumsum_freqs = torch.cumsum(torch.tensor(norm_freqs, dtype=torch.float), dim=0)\n",
    "    \n",
    "    # Initialize batch accumulators\n",
    "    target_batch = torch.zeros(batch_size, dtype=torch.long)\n",
    "    context_batch = torch.zeros((batch_size, 1 + num_ns), dtype=torch.long)\n",
    "    batch_index = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)\n",
    "        for target_index, target_word_id in enumerate(sentence):\n",
    "            # Randomly choose the window size\n",
    "            dynamic_window = np.random.randint(1, window+1)\n",
    "            context_start = max(0, target_index - dynamic_window)\n",
    "            context_end = min(sentence_length, target_index + dynamic_window + 1)\n",
    "            context_indices = [i for i in range(context_start, context_end) if i != target_index]\n",
    "            \n",
    "            for context_index in context_indices:\n",
    "                if batch_index == batch_size:\n",
    "                    yield target_batch.clone(), context_batch.clone()\n",
    "                    target_batch.zero_()\n",
    "                    context_batch.zero_()\n",
    "                    batch_index = 0  # Reset for next batch\n",
    "                    \n",
    "                # Assign target word ID\n",
    "                target_batch[batch_index] = target_word_id\n",
    "                \n",
    "                \n",
    "                negative_samples = torch.searchsorted(cumsum_freqs, torch.rand(num_ns)).tolist()\n",
    "                context_words = [sentence[context_index]] + negative_samples\n",
    "                context_batch[batch_index] = torch.tensor(context_words, dtype=torch.long)\n",
    "                batch_index += 1\n",
    "\n",
    "    if batch_index > 0:\n",
    "        yield target_batch[:batch_index], context_batch[:batch_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should comply with the following specification:\n",
    "\n",
    "**training_examples** (*vocab*, *counts*, *sentences*, *window* = 5, *num_ns* = 5, *batch_size* = 524,288, *ns_exponent*=0.75)\n",
    "\n",
    "> Reads from an iterable of *sentences* (lists of string tokens), preprocesses them using the function implemented in Problem&nbsp;2, and then yields pairs of input batches for gradient-based training, represented as described above. Each batch contains *batch_size* positive examples. The parameter *window* specifies the maximal distance between a target word and a context word in a positive example; the actual window size around any given target word is sampled uniformly at random. The parameter *num_ns* specifies the number of negative samples per positive sample. The parameter *ns_exponent* specifies the exponent in the negative sampling (called $\\alpha$ above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§û Test your code\n",
    "\n",
    "To test your code, compare the total number of positive samples (across all batches) to the total number of tokens in the (un-preprocessed) minimal dataset. The ratio between these two values should be ca. 2.64. If you can spare the time, you can make the same comparison on the full dataset; here, the expected ratio is 3.25. As before, the numbers may vary slightly because of randomness, so you may want to run the comparison more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.626771653543307\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "vocab = vocab_minimal\n",
    "counts = counts_minimal\n",
    "sentences = preprocessed_sentences_minimal\n",
    "\n",
    "total_positive_samples = 0\n",
    "total_tokens = total_counts_minimal_all\n",
    "\n",
    "for target_batch, context_batch in training_examples(vocab, counts, sentences):\n",
    "    total_positive_samples += len(target_batch)\n",
    "\n",
    "# Calculate the ratio between total positive samples and total tokens\n",
    "ratio = total_positive_samples / total_tokens\n",
    "\n",
    "print(ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170815"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_counts_minimal_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.254216381635913\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "vocab = vocab_full\n",
    "counts = counts_full\n",
    "sentences = preprocessed_sentences_full\n",
    "\n",
    "total_positive_samples = 0\n",
    "total_tokens = total_counts_full_all\n",
    "\n",
    "for target_batch, context_batch in training_examples(vocab, counts, sentences):\n",
    "    total_positive_samples += len(target_batch)\n",
    "\n",
    "ratio = total_positive_samples / total_tokens\n",
    "\n",
    "print(ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Implement the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to implement the skip-gram model as such. The cell below contains skeleton code for this. As you will recall from Lecture&nbsp;2.4, the core of the implementation is formed by two embedding layers: one for the target word representations, and one for the context word representations. Your task is to implement the missing `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SGNSModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab, embedding_dim):\n",
    "        super(SGNSModel, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        vocab_size = len(vocab)\n",
    "        self.w = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.c = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, w, c):\n",
    "        w_embed = self.w(w) #Shape: [B, embedding_dim]\n",
    "        c_embed = self.c(c) #Shape: [B, k+1, embedding_dim]\n",
    "        \n",
    "        dot_products = torch.bmm(w_embed.unsqueeze(1), c_embed.transpose(1, 2)).squeeze(1)\n",
    "        #BMM for Batch dot product\n",
    "        \n",
    "        return dot_products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation of the `forward()` method should comply with the following specification:\n",
    "\n",
    "**forward** (*self*, *w*, *c*)\n",
    "\n",
    "> The input to this methods is a tensor *w* with target words of shape $[B]$ and a tensor *c* with context words of shape $[B, 1+k]$, where $B$ is the batch size and $k$ is the number of negative samples. The two tensors are structured as explained for Problem&nbsp;3. The output of the method is a tensor $D$ of shape $[B, k+1]$ where entry $D_{ij}$ is the dot product between the embedding vector for the $i$th target word and the embedding vector for the context word in row $i$, column $j$.\n",
    "\n",
    "**üí° Hint:** To compute a dot product $x^\\top y$, you can first compute the Hadamard product $z = x \\odot y$ and then sum up the elements of $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§û Test your code\n",
    "\n",
    "Test your code by creating an instance of the model, and check that `forward` returns the expected result on random input tensors *w* and *c*. To help you, the following function will return a random example from the first 100 examples produced by `training_examples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_example(vocab, counts, sentences):\n",
    "    skip = np.random.randint(100)\n",
    "    for i, example in enumerate(training_examples(vocab, counts, sentences, num_ns=1, batch_size=5)):\n",
    "        if i >= skip:\n",
    "            break\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed! Output shape is correct.\n",
      "Output: tensor([[  5.2916, -11.1369],\n",
      "        [ -0.8718,  -6.2550],\n",
      "        [-15.3995, -11.1504],\n",
      "        [ 15.7195,   3.4277],\n",
      "        [  4.0296,  -1.5676]], grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17456\\AppData\\Local\\Temp\\ipykernel_40800\\3326159455.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_tensor = torch.tensor(w, dtype=torch.long)\n",
      "C:\\Users\\17456\\AppData\\Local\\Temp\\ipykernel_40800\\3326159455.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c_tensor = torch.tensor(c, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "model = SGNSModel(vocab_minimal,embedding_dim = 100 )\n",
    "\n",
    "mini_dataset_indexed = [[vocab_minimal[word] for word in sentence if word in vocab_minimal] for sentence in mini_dataset]\n",
    "\n",
    "w, c = random_example(vocab_minimal, counts_minimal, mini_dataset_indexed)\n",
    "\n",
    "w_tensor = torch.tensor(w, dtype=torch.long)\n",
    "c_tensor = torch.tensor(c, dtype=torch.long)\n",
    "\n",
    "output = model.forward(w_tensor, c_tensor)\n",
    "\n",
    "expected_shape = (w_tensor.size(0), c_tensor.size(1))\n",
    "assert output.shape == expected_shape, f\"Output shape {output.shape} does not match expected shape {expected_shape}.\"\n",
    "\n",
    "print(\"Test passed! Output shape is correct.\")\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a working model, it is time to train it. The training loop for the skip-gram model will be very similar to the prototypical training loop that you already know from previous notebooks, with two things to note:\n",
    "\n",
    "First, instead of categorical cross entropy, you will use binary cross entropy. Just like the standard implementation of the softmax classifier, the skip-gram model does not include a final non-linearity, so you should use [`binary_cross_entropy_with_logits()`](https://pytorch.org/docs/1.9.1/generated/torch.nn.functional.binary_cross_entropy_with_logits.html).\n",
    "\n",
    "The second thing to note is that you will have to create the tensor with the output labels, as explained already in Problem&nbsp;3. This should be a matrix of size $[B, 1+k]$ whose first column contains $1$s and whose remaining columns contains $0$s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is skeleton code for the training loop, including default values for the most important hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(dataset, embedding_dim=50, window=5, num_ns=5, batch_size=1<<20, n_epochs=1, lr=0.1):\n",
    "\n",
    "    vocab, counts = make_vocab_and_counts(dataset)\n",
    "    model = SGNSModel(vocab, embedding_dim)\n",
    "    preprocessed_sentences = list(preprocess(vocab, counts, dataset))\n",
    "    \n",
    "    # Initialize the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        # Ensure sentences_indexed is used and produced by make_vocab_and_counts beforehand\n",
    "        for target, context in training_examples(vocab, counts, preprocessed_sentences):\n",
    "            # Check if target and context are already tensors\n",
    "            if not isinstance(target, torch.Tensor):\n",
    "                target = torch.tensor(target, dtype=torch.long)\n",
    "            if not isinstance(context, torch.Tensor):\n",
    "                context = torch.tensor(context, dtype=torch.long)\n",
    "            # Create output labels tensor\n",
    "            labels = torch.zeros(context.size(0), context.size(1))\n",
    "            labels[:, 0] = 1  # First column for positive examples\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(target, context)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = F.binary_cross_entropy_with_logits(predictions, labels)\n",
    "            print(loss.item())\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show you how `train` is meant to be used, the code in the next cell trains a model on the minimal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.904158353805542\n",
      "Epoch [1/10], Loss: 2.9042\n",
      "2.479555130004883\n",
      "Epoch [2/10], Loss: 2.4796\n",
      "2.105211019515991\n",
      "Epoch [3/10], Loss: 2.1052\n",
      "1.7880841493606567\n",
      "Epoch [4/10], Loss: 1.7881\n",
      "1.5040004253387451\n",
      "Epoch [5/10], Loss: 1.5040\n",
      "1.2471671104431152\n",
      "Epoch [6/10], Loss: 1.2472\n",
      "1.016473650932312\n",
      "Epoch [7/10], Loss: 1.0165\n",
      "0.8292652368545532\n",
      "Epoch [8/10], Loss: 0.8293\n",
      "0.7023128271102905\n",
      "Epoch [9/10], Loss: 0.7023\n",
      "0.6384559273719788\n",
      "Epoch [10/10], Loss: 0.6385\n"
     ]
    }
   ],
   "source": [
    "model = train(mini_dataset, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§û Test your code\n",
    "\n",
    "Test your implementation of the training loop by training a model on the minimal dataset. This should only take a few seconds. You will not get useful word vectors, but you will be able to see whether your code runs without errors.\n",
    "\n",
    "Once you have passed this test, you can train a model on the full dataset. Print the loss to check that the model is actually learning; if the loss is not decreasing, try to find the problem before wasting time (and energy) on useless training.\n",
    "\n",
    "Training on the full dataset will take some time ‚Äì on a CPU, you should expect 10‚Äì40 minutes per epoch, depending on hardware. To give you some guidance: The total number of positive examples is approximately 58M, and the batch size is chosen so that each batch contains roughly 10% of these examples. To speed things up, you can train using a GPU; our reference implementation runs in less than 2 minutes per epoch on [Colab](http://colab.research.google.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.880861282348633\n",
      "2.567476987838745\n",
      "2.328270196914673\n",
      "2.0846574306488037\n",
      "1.8627952337265015\n",
      "1.6731767654418945\n",
      "1.495949387550354\n",
      "1.3428343534469604\n",
      "1.1858158111572266\n",
      "1.0549097061157227\n",
      "0.9506063461303711\n",
      "0.8579502105712891\n",
      "0.779617965221405\n",
      "0.736412763595581\n",
      "0.6878771781921387\n",
      "0.6618800759315491\n",
      "0.6341748833656311\n",
      "0.6050359010696411\n",
      "0.5972283482551575\n",
      "0.5803269743919373\n",
      "0.5648996233940125\n",
      "0.5540962815284729\n",
      "0.5443975329399109\n",
      "0.5361320376396179\n",
      "0.5296789407730103\n",
      "0.5369572639465332\n",
      "0.5205166935920715\n",
      "0.5224615335464478\n",
      "0.5150646567344666\n",
      "0.5097680687904358\n",
      "0.5010051727294922\n",
      "0.4969541132450104\n",
      "0.4989548623561859\n",
      "0.48888882994651794\n",
      "0.4892235994338989\n",
      "0.48369288444519043\n",
      "0.4751012325286865\n",
      "0.488967627286911\n",
      "0.4717646539211273\n",
      "0.4746742248535156\n",
      "0.47321367263793945\n",
      "0.48081982135772705\n",
      "0.47351399064064026\n",
      "0.46241891384124756\n",
      "0.46162691712379456\n",
      "0.4587744176387787\n",
      "0.48168084025382996\n",
      "0.45764294266700745\n",
      "0.453471302986145\n",
      "0.46322011947631836\n",
      "0.4540613889694214\n",
      "0.44270145893096924\n",
      "0.4378797113895416\n",
      "0.4504002034664154\n",
      "0.44772887229919434\n",
      "0.4606967866420746\n",
      "0.4593825042247772\n",
      "0.4417465925216675\n",
      "0.44148755073547363\n",
      "0.44386371970176697\n",
      "0.4404188394546509\n",
      "0.4469149112701416\n",
      "0.4426852762699127\n",
      "0.42762795090675354\n",
      "0.43896397948265076\n",
      "0.4363304078578949\n",
      "0.4381628930568695\n",
      "0.434109091758728\n",
      "0.4368482530117035\n",
      "0.43968942761421204\n",
      "0.4325982630252838\n",
      "0.42925313115119934\n",
      "0.433493047952652\n",
      "0.4349087178707123\n",
      "0.4332449436187744\n",
      "0.42834949493408203\n",
      "0.42669931054115295\n",
      "0.42636415362358093\n",
      "0.4325812757015228\n",
      "0.42551299929618835\n",
      "0.42663300037384033\n",
      "0.42365846037864685\n",
      "0.4203648865222931\n",
      "0.42231476306915283\n",
      "0.4218308925628662\n",
      "0.4171483814716339\n",
      "0.4256936013698578\n",
      "0.42188599705696106\n",
      "0.4273543357849121\n",
      "0.4265305995941162\n",
      "0.42337027192115784\n",
      "0.42115703225135803\n",
      "0.4171108305454254\n",
      "0.4243314266204834\n",
      "0.4230314791202545\n",
      "0.4255772531032562\n",
      "0.4298057556152344\n",
      "0.4345822036266327\n",
      "0.42457249760627747\n",
      "0.42001447081565857\n",
      "0.42134857177734375\n",
      "0.4297916889190674\n",
      "0.4219895303249359\n",
      "0.42354848980903625\n",
      "0.42068246006965637\n",
      "0.4157832860946655\n",
      "0.4165785014629364\n",
      "0.41913077235221863\n",
      "0.4240090548992157\n",
      "0.4202755093574524\n",
      "Epoch [1/2], Loss: 66.2923\n",
      "0.40689924359321594\n",
      "0.4109393060207367\n",
      "0.42086291313171387\n",
      "0.4207686483860016\n",
      "0.4150939881801605\n",
      "0.4209872782230377\n",
      "0.4174695312976837\n",
      "0.42026999592781067\n",
      "0.41443657875061035\n",
      "0.4144485294818878\n",
      "0.420156866312027\n",
      "0.4252139627933502\n",
      "0.4152027666568756\n",
      "0.41534873843193054\n",
      "0.41153526306152344\n",
      "0.4144526720046997\n",
      "0.4127277433872223\n",
      "0.4127049744129181\n",
      "0.4119516611099243\n",
      "0.4101232588291168\n",
      "0.41090521216392517\n",
      "0.4084332287311554\n",
      "0.4090545177459717\n",
      "0.4069701135158539\n",
      "0.40907368063926697\n",
      "0.41162851452827454\n",
      "0.40981999039649963\n",
      "0.4093994200229645\n",
      "0.4102156162261963\n",
      "0.4082851707935333\n",
      "0.4056507647037506\n",
      "0.4075448513031006\n",
      "0.4068722724914551\n",
      "0.40344324707984924\n",
      "0.40868744254112244\n",
      "0.4087117910385132\n",
      "0.40248262882232666\n",
      "0.41312870383262634\n",
      "0.4050844609737396\n",
      "0.40568041801452637\n",
      "0.40646958351135254\n",
      "0.4117494821548462\n",
      "0.40982306003570557\n",
      "0.40269672870635986\n",
      "0.4050898849964142\n",
      "0.4051080048084259\n",
      "0.41380563378334045\n",
      "0.4041033685207367\n",
      "0.40324947237968445\n",
      "0.4100308120250702\n",
      "0.4049895703792572\n",
      "0.3988450765609741\n",
      "0.3957558870315552\n",
      "0.40229567885398865\n",
      "0.4013585150241852\n",
      "0.41714906692504883\n",
      "0.4129682779312134\n",
      "0.4013579189777374\n",
      "0.4026156961917877\n",
      "0.4080137014389038\n",
      "0.4050099849700928\n",
      "0.40712234377861023\n",
      "0.40380141139030457\n",
      "0.39856263995170593\n",
      "0.40407371520996094\n",
      "0.40148571133613586\n",
      "0.4043227732181549\n",
      "0.40140673518180847\n",
      "0.40406712889671326\n",
      "0.4076138436794281\n",
      "0.4019569456577301\n",
      "0.3996509611606598\n",
      "0.40423405170440674\n",
      "0.4061831533908844\n",
      "0.4038737118244171\n",
      "0.40188995003700256\n",
      "0.4000803232192993\n",
      "0.39952048659324646\n",
      "0.40601399540901184\n",
      "0.39841675758361816\n",
      "0.40069496631622314\n",
      "0.39981210231781006\n",
      "0.39843448996543884\n",
      "0.40158209204673767\n",
      "0.40009650588035583\n",
      "0.39387187361717224\n",
      "0.4015132486820221\n",
      "0.3994620144367218\n",
      "0.40545645356178284\n",
      "0.4057847261428833\n",
      "0.40171003341674805\n",
      "0.4014509916305542\n",
      "0.39929643273353577\n",
      "0.40178897976875305\n",
      "0.4014921486377716\n",
      "0.40244999527931213\n",
      "0.41035377979278564\n",
      "0.4130140244960785\n",
      "0.40715253353118896\n",
      "0.4013836681842804\n",
      "0.4019191265106201\n",
      "0.40654250979423523\n",
      "0.4067256450653076\n",
      "0.40408745408058167\n",
      "0.40100935101509094\n",
      "0.3996760845184326\n",
      "0.4014393985271454\n",
      "0.4004460573196411\n",
      "0.4037618935108185\n",
      "0.3804382383823395\n",
      "Epoch [2/2], Loss: 44.7023\n"
     ]
    }
   ],
   "source": [
    "# TODO: Train your model on the full dataset here\n",
    "model = train(full_dataset, n_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Analyse the embeddings (reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a trained model, you will probably be curious to see what it has learned. You can inspect your embeddings using the [Embedding Projector](http://projector.tensorflow.org). To that end, click on the ‚ÄòLoad‚Äô button, which will open up a dialogue with instructions for how to upload embeddings from your computer.\n",
    "\n",
    "You will need to upload two tab-separated files. To create them, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    # Extract the embedding vectors as a NumPy array\n",
    "    embeddings = model.w.weight.detach().numpy()\n",
    "    \n",
    "    # Create the word‚Äìvector pairs\n",
    "    items = sorted((i, w) for w, i in model.vocab.items())\n",
    "    items = [(w, e) for (i, w), e in zip(items, embeddings)]\n",
    "    \n",
    "    # Write the embeddings and the word labels to files\n",
    "    with open('vectors.tsv', 'wt',encoding = 'utf-8') as fp1, open('metadata.tsv', 'wt',encoding = 'utf-8') as fp2:\n",
    "        for w, e in items:\n",
    "            print('\\t'.join('{:.5f}'.format(x) for x in e), file=fp1)\n",
    "            print(w, file=fp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some time to explore the embedding space. In particular, inspect the local neighbourhoods of words that you are curious about, say the 10 closest neighbours. Document your exploration in a short reflection piece (ca. 150&nbsp;words). Respond to the following prompts:\n",
    "\n",
    "* Which words did you try? Which results did you get? Did you do anything else than inspecting local neighbourhoods?\n",
    "* Based on what you know about word embeddings, did you expect your results? How do you explain them?\n",
    "* What did you learn? How, exactly, did you learn it? Why does this learning matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We tried Stockholm first, here's the result:**\n",
    "\n",
    "- **Helsinki**: 0.262\n",
    "- **Sweden**: 0.330\n",
    "- **London**: 0.342\n",
    "- **Oslo**: 0.347\n",
    "- **Brussels**: 0.365\n",
    "- **Gothenburg**: 0.373\n",
    "- **December**: 0.377\n",
    "- **Copenhagen**: 0.378\n",
    "- **At**: 0.396\n",
    "- **Suburb**: 0.398\n",
    "\n",
    "**Then we tried football:**\n",
    "\n",
    "- **Footballer**: 0.224\n",
    "- **Basketball**: 0.239\n",
    "- **Soccer**: 0.247\n",
    "- **Club**: 0.249\n",
    "- **Player**: 0.270\n",
    "- **Goalkeeper**: 0.275\n",
    "- **Tennis**: 0.333\n",
    "- **Japanese**: 0.337\n",
    "- **Retired**: 0.340\n",
    "- **Handball**: 0.341\n",
    "\n",
    "**And then we tried 'Isolate 11 points' to have a clear view.** Later we squeeze the data with PCA into 2D. To be specific, we set X 'Component#2', and found that Gothenburg, Sweden are pointing to the same direction as Stockholm does.\n",
    "\n",
    "**Yes, we expect the results we observed.** After training, the word embeddings are able to show the contextual relationship and connection information. So just like what we saw in the first question, words with similar meanings are close to each other in the embedding space. Because we found that in the training process, we let the model learn the similarity of contextual words through embeddings. So in the visualization, contextual words are close.\n",
    "\n",
    "**What did you learn? How, exactly, did you learn it? Why does this learning matter?**\n",
    "\n",
    "We learn skip gram model in general, and learned the details of this model through programming, by watching lectures, and looking up information online. Because we can use it to embed words and even develop our own models. Moreover, last week we used the existed trained embedding layers, this week we train it by ourselves. We believe this experience is important to a continuous learning process in NLP.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëç Well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
