{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1: Language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will implement and train two neural language models: the fixed-window model and the recurrent neural network model. You will evaluate these models by computing their perplexity on a benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you should use the GPU if you have one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this lab is [WikiText](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/), a collection of more than 100 million tokens extracted from the ‚ÄúGood‚Äù and ‚ÄúFeatured‚Äù articles on Wikipedia. We will use the small version of the dataset, which contains slightly more than 2.5 million tokens.\n",
    "\n",
    "The next cell contains code for an object that will act as a container for the ‚Äútraining‚Äù and the ‚Äúvalidation‚Äù section of the data. We fill this container by reading the corresponding text files. The only processing we do is to whitespace-tokenise, and to enclose each non-empty line within `<bos>` (beginning-of-sentence) and `<eos>` (end-of-sentence) tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiText(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.train = self.read_data('wiki.train.tokens')\n",
    "        self.valid = self.read_data('wiki.valid.tokens')\n",
    "    \n",
    "    def read_data(self, path):\n",
    "        ids = []\n",
    "        with open(path, encoding='utf-8') as source:\n",
    "            for line in source:\n",
    "                line = line.rstrip()\n",
    "                if line:\n",
    "                    for token in ['<bos>'] + line.split() + ['<eos>']:\n",
    "                        if token not in self.vocab:\n",
    "                            self.vocab[token] = len(self.vocab)\n",
    "                        ids.append(self.vocab[token])\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below loads the data and prints the total number of tokens and the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in train: 2099444\n",
      "Tokens in valid: 218808\n",
      "Vocabulary size: 33279\n"
     ]
    }
   ],
   "source": [
    "wikitext = WikiText()\n",
    "print('Tokens in train:', len(wikitext.train))\n",
    "print('Tokens in valid:', len(wikitext.valid))\n",
    "print('Vocabulary size:', len(wikitext.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Fixed-window model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement and train the fixed-window neural language model proposed by [Bengio et al. (2003)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) and presented in the lectures. Recall that an input to the network takes the form of a vector of $n-1$ integers representing the preceding words. Each integer is mapped to a vector via an embedding layer. (All positions share the same embedding.) The embedding vectors are then concatenated and sent through a two-layer feed-forward network with a non-linearity in the form of a rectified linear unit (ReLU) and a final softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.1: Vectorise the data (1&nbsp;point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to write code for transforming the data in the WikiText container into a vectorised form that can be fed to the fixed-window model. Concretely, you will implement a [collate function](https://pytorch.org/docs/stable/data.html#dataloader-collate-fn) in the form of a callable vectoriser object. Complete the skeleton code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowVectorizer(object):\n",
    "    def __init__(self, n):\n",
    "        # n-gram order\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # TODO: Replace the following line with your own code\n",
    "        N = len(data) - (self.n-1) # calculate how many n-grams in the dataset \n",
    "\n",
    "        X = [data[i:i+self.n-1] for i in range(N)]\n",
    "        y = [data[i+self.n-1] for i in range(N) ]                               # [ , )\n",
    "        return torch.tensor(X), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should implement the following specification:\n",
    "\n",
    "**__init__** (*self*, *n*)\n",
    "\n",
    "> Creates a new vectoriser with n-gram order $n$. Your code should be able to handle arbitrary n-gram orders $n \\geq 1$.\n",
    "\n",
    "**__call__** (*self*, *data*)\n",
    "\n",
    "> Transforms WikiText *data* (a list of word ids) into a pair of tensors $\\mathbf{X}$, $\\mathbf{y}$ that can be used to train the fixed-window model. Let $N$ be the total number of $n$-grams from the token list; then $\\mathbf{X}$ is a matrix with shape $(N, n-1)$ and $\\mathbf{y}$ is a vector with length $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your implementation by running the code in the next cell. Does the output match your expectation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x, valid_y = FixedWindowVectorizer(2)(wikitext.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = FixedWindowVectorizer(2)(wikitext.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2: Implement the model (2&nbsp;points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to implement the fixed-window model based on the graphical specification given in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n, n_words, embedding_dim=50, hidden_dim=50):\n",
    "        # n_words -- number of words in vocabulary\n",
    "        # n       -- n-gram\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_words,embedding_dim)\n",
    "        self.fc = nn.Linear((n - 1) * embedding_dim, hidden_dim) \n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.out = nn.Linear(hidden_dim, n_words)   \n",
    "        # TODO: Add your own code\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.view(x.size(0), -1)\n",
    "        hidden = self.fc(embeds)\n",
    "        hidden = self.ReLU(hidden)\n",
    "        output = self.out(hidden) # output is one-hot code for every word in vocablary\n",
    "        probs = F.softmax(output, dim=1)        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the two methods:\n",
    "\n",
    "**__init__** (*self*, *n*, *n_words*, *embedding_dim*=50, *hidden_dim*=50)\n",
    "\n",
    "> Creates a new fixed-window neural language model. The argument *n* specifies the model&rsquo;s $n$-gram order. The argument *n_words* is the number of words in the vocabulary. The arguments *embedding_dim* and *hidden_dim* specify the dimensionalities of the embedding layer and the hidden layer of the feedforward network, respectively; their default value is 50.\n",
    "\n",
    "**forward** (*self*, *x*)\n",
    "\n",
    "> Computes the network output on an input batch *x*. The shape of *x* is $(B, n-1)$, where $B$ is the batch size. The output of the forward pass is a tensor of shape $(B, V)$ where $V$ is the number of words in the vocabulary.\n",
    "\n",
    "**Hint:** The most efficient way to implement the vector concatenation in this model is to use the [`view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) method.\n",
    "\n",
    "#### ü§û Test your code\n",
    "\n",
    "Test your code by instantiating the model and feeding it a batch of examples from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0043, -0.1411,  0.1287,  ..., -0.0634,  0.0798,  0.0381],\n",
      "        [ 0.2523,  0.0688,  0.2223,  ..., -0.0818,  0.3047,  0.5258],\n",
      "        [-0.1861,  0.2528,  0.0870,  ..., -0.2823, -0.0673,  0.0491],\n",
      "        [-0.5386, -0.3678, -0.7150,  ..., -0.3137,  0.4225,  0.0059],\n",
      "        [ 0.1723,  0.2063,  0.0388,  ..., -0.0166,  0.1142, -0.0798]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_words = len(wikitext.vocab)  \n",
    "model = FixedWindowModel(2, n_words)\n",
    "\n",
    "batch_size = 5\n",
    "train_x_batch = train_x[:batch_size]\n",
    "\n",
    "output = model(train_x_batch)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3: Train the model (3&nbsp;points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your final task is to write code to train the fixed-window model using minibatch gradient descent and the cross-entropy loss function. This should be a straightforward generalisation of the training loops that you have seen so far. Complete the skeleton code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size):\n",
    "    for i in range(0, len(data) - batch_size + 1, batch_size):\n",
    "        yield data[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixed_window(n, n_epochs=2, batch_size=3200, lr=0.01):\n",
    "    n_words = len(wikitext.vocab)\n",
    "    model = FixedWindowModel(n, n_words)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    total_loss = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()  # ËÆæÁΩÆÊ®°Âûã‰∏∫ËÆ≠ÁªÉÊ®°Âºè\n",
    "        for batch_x, batch_y in zip(get_batches(train_x, batch_size), get_batches(train_y, batch_size)):\n",
    "            # batch_x Âíå batch_y ÊòØÈÖçÂØπÁöÑËæìÂÖ•ÂíåÁõÆÊ†á\n",
    "\n",
    "            # Ê∏ÖÁ©∫Ê¢ØÂ∫¶\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # ÂâçÂêë‰º†Êí≠\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # ËÆ°ÁÆóÊçüÂ§±\n",
    "            loss = F.cross_entropy(outputs, batch_y)\n",
    "            print(\"train_loss: \",loss)\n",
    "            # ÂèçÂêë‰º†Êí≠Âíå‰ºòÂåñ\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    train_loss = total_loss/len(train_x)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x,batch_y in zip(get_batches(valid_x, batch_size), get_batches(valid_y, batch_size)):\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.long)\n",
    "            outputs = model(batch_x)\n",
    "            total_loss += F.cross_entropy(outputs, batch_y, reduction='sum').item()\n",
    "\n",
    "        # ËÆ°ÁÆóÈ™åËØÅÈõÜ‰∏äÁöÑÂπ≥ÂùáÊçüÂ§±\n",
    "    valid_loss = total_loss / len(valid_x)\n",
    "    perplexity = torch.exp(torch.tensor(valid_loss))\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Validation Perplexity: {perplexity}') \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the training function:\n",
    "\n",
    "**train_fixed_window** (*n*, *n_epochs* = 1, *batch_size* = 3200, *lr* = 0.01)\n",
    "\n",
    "> Trains a fixed-window neural language model of order *n* using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*. After each epoch, prints the perplexity of the model on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below trains a bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(10.4342, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(10.2747, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(10.1096, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(9.9067, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(9.7315, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(9.3465, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(9.2265, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(8.9221, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(8.5131, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(8.3963, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(8.0415, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.8691, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.8227, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.7787, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.7726, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.5043, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.5083, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.4016, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.2284, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.2508, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.3654, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.3765, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.5562, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.4261, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.1498, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.1138, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0574, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.1952, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9295, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.2396, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.1594, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0696, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0510, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0200, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0406, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.1488, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0529, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0113, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9966, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8972, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8040, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0816, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8791, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0430, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9616, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9477, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7827, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6534, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9748, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9212, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9520, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5512, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7636, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7352, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6841, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8372, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7643, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6217, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6263, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9582, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9069, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4734, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6729, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6433, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4920, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3904, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5717, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0041, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0492, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6271, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8255, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8909, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8893, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6885, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8067, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7332, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8188, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8056, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7912, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8098, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6671, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9399, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7693, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5905, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6656, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5418, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5064, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7102, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6516, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6235, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8673, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6701, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8318, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0109, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9812, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8447, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6603, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6755, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8918, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8864, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5349, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6032, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9529, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7949, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8285, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6479, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8822, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6061, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8408, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5909, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8815, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7492, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8027, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7868, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6333, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6851, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6392, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7992, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8342, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5191, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6494, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5238, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4997, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5810, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8827, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6273, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7152, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4830, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3963, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3192, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2477, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3773, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5474, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5907, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5164, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4841, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5589, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4403, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6437, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6079, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6745, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5957, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5261, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9290, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5658, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3406, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5091, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7528, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6938, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4865, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3987, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5774, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7150, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7074, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5051, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1731, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3904, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6749, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4943, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4139, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6069, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4504, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3066, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4289, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6075, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7462, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5063, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3006, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2789, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7933, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0405, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6374, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4899, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3848, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5641, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5805, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4856, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5817, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6497, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6972, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7183, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4606, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5854, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5176, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5642, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1655, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2701, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3758, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7504, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2959, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6752, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6566, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4427, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2372, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5042, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4099, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4925, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3741, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7476, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5394, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5206, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3002, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5599, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4286, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6133, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7300, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4656, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5423, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2811, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3293, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3598, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4791, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4440, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5095, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9669, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7127, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7196, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7230, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6147, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5570, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2785, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6842, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4757, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5173, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4287, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5346, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3414, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4556, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4333, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4615, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1574, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4404, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2847, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2997, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0495, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3574, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6104, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5064, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3841, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5164, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5208, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3182, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4393, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3244, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3969, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6184, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4803, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4420, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4421, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2734, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2852, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0290, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0799, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4783, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3613, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7391, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5454, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4984, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6196, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3010, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6149, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5519, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4462, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5301, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2218, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2207, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8598, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0254, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1003, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6027, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8091, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5561, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5656, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7098, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5721, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5043, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4708, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0853, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1405, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4943, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5354, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5146, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4914, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5202, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4680, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4046, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4318, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4165, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3168, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2962, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3468, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0856, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2051, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3506, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5791, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5559, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4524, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4795, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5342, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5184, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3725, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6709, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6130, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4595, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4610, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2347, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2590, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2263, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1807, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2624, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6858, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5500, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6418, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5894, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2317, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6866, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2437, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2749, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4129, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2928, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2553, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1103, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2154, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1701, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1083, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1716, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5850, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5612, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1746, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3716, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2189, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3584, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3545, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2553, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1331, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2722, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1741, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2750, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5007, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2441, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0693, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8227, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6586, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4858, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4789, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3460, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5343, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5781, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4816, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4731, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3716, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6362, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4460, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8009, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1991, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2990, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3573, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2519, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3883, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3083, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2311, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2462, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2392, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4521, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5604, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2277, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0850, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1741, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2807, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7007, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2177, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3167, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2347, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2505, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4542, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4417, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2821, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2920, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2876, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1312, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0420, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3570, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1473, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0456, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1164, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1400, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4323, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2753, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1515, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3650, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1638, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2787, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4512, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4741, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4261, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1639, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3968, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2699, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3292, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5493, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4702, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2253, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2690, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2446, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4149, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8192, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9212, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5787, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4030, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2060, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0370, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3340, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3638, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2953, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4009, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3838, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2454, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3566, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1965, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3201, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3496, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4725, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2390, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3868, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0876, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4218, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4128, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4610, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1704, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0777, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0661, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5029, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4527, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3892, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3108, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2747, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3562, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9025, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1468, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4381, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5312, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4079, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3732, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4154, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4508, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1532, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1523, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7600, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4223, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4001, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4087, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1360, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5139, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3182, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1293, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1556, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9837, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0900, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1833, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3002, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4022, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5218, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4055, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4028, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2168, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8522, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9247, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2825, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2145, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2580, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3468, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3529, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5234, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2784, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2450, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9478, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3911, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0909, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4328, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0617, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1630, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3963, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2832, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1939, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2722, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3230, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2012, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3566, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4217, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1432, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0353, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3686, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3857, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1378, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2048, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2663, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1660, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7470, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5602, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2044, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0995, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0055, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9250, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0508, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3541, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3682, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2968, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9371, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4087, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1964, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1638, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1496, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7068, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1555, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1862, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8451, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3779, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2528, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3465, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2228, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5526, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0922, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0565, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1335, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1802, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1738, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2086, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0601, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3129, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6169, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4768, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4456, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4896, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3485, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1826, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2015, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3169, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4169, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2423, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2022, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1628, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4270, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2854, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5200, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7030, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5846, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4390, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2628, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0198, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1566, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1049, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4783, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5191, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4112, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2702, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4058, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1869, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2274, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4162, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0641, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2473, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5427, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4936, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4840, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0473, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1206, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2210, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2265, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0979, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2987, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2326, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0358, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1920, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2286, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4629, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1948, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1988, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5707, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3556, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4086, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0939, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2186, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0851, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1269, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1904, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0458, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3489, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3067, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3660, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2592, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5717, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3176, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2799, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2903, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2371, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2547, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1271, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2542, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3126, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2003, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1152, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4251, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4316, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4614, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1251, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9990, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2030, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1156, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8135, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7143, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1283, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0445, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3857, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1234, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2776, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0078, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6517, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4995, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3781, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5151, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3974, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2628, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2751, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3717, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3466, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4381, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4868, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4052, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1832, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8846, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2921, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2488, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2852, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3567, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2448, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2229, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3694, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1975, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3708, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5656, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4038, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2340, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3175, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2900, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3176, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6413, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1857, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2379, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4690, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3131, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3060, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3454, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3411, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1414, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2526, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0433, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0528, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8536, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0262, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1033, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9767, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6568, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8123, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9151, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8442, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8843, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8838, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0126, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0800, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1352, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1168, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8897, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8662, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9288, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8019, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7257, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8506, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8425, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0165, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8940, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7466, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8695, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8666, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9035, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6738, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8592, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9053, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7546, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8165, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8428, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8177, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9561, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8092, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8108, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8488, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7730, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6843, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8704, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7517, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9658, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6699, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9063, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7788, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5795, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9264, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9867, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0541, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5929, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7972, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8402, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7232, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8167, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8005, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6230, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6628, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9500, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8790, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5604, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8061, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7025, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6673, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5068, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5215, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0915, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0638, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7043, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8773, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8442, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9332, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7816, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9289, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8066, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8746, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9442, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8221, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9959, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7549, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0824, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9057, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7797, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9398, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8688, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8417, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9807, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8518, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7939, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9747, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8709, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0402, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1217, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1270, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0040, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8021, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7059, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8295, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9662, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7370, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8336, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2441, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9568, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0560, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9520, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3076, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8722, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9593, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7313, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1105, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0267, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1109, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8630, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8751, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9746, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9791, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0188, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9452, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7624, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9222, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7978, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7146, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9631, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2406, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9266, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0188, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8346, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7705, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6665, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6012, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7585, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8174, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9250, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8530, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8243, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9130, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6808, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8717, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9439, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0351, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9546, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8381, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1741, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8705, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6739, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8524, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9866, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9842, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7756, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6330, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8020, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9544, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9826, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8320, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4660, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7464, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8544, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4559, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8241, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9182, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6695, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6275, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7695, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9577, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0549, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8159, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6707, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6451, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0553, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2815, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8983, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7932, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6655, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8492, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8488, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7688, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9626, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0610, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9528, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0020, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7777, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9581, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9124, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9439, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4962, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5827, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7391, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0715, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6284, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9351, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0097, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8286, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6199, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9108, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7609, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7203, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7097, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0993, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9489, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9553, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7472, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8803, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8388, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0478, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1292, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9298, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9894, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8059, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6712, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8097, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8350, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7789, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8893, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2913, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0176, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9877, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1390, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9432, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9759, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7049, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9825, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8623, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8690, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8308, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9616, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7723, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9296, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8127, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8621, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6018, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8234, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7020, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7485, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4695, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7704, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0244, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8455, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8337, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9876, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9120, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7141, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8640, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7310, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8552, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0831, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9645, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7486, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8349, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7590, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8143, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5996, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5800, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8572, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7017, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1141, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9638, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9498, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0801, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7013, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0439, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9643, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8830, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9849, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6164, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6904, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.3784, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4683, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5821, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0265, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0826, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9784, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9510, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1182, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9931, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9341, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7726, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5715, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6854, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9744, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9701, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9760, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9417, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9793, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8971, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8328, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8683, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7968, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7257, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7735, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8046, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5707, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6275, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8556, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9799, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9558, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9595, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9619, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0015, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9879, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7640, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9953, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0057, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8852, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9473, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5422, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6590, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7772, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6852, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7231, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0189, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9370, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9382, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8981, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6697, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1545, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7501, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7261, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9418, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8445, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8688, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6605, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7753, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7064, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6739, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6451, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1321, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0716, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7164, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7730, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6282, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8382, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9027, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7552, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5802, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7933, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7906, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7916, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8556, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7562, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5774, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2284, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0664, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9196, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9742, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8012, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0273, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0922, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9985, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9509, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7987, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9965, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8960, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2106, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7691, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8558, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8882, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7297, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9544, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8609, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8117, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8032, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7727, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9415, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1010, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7596, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6223, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7686, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8895, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0569, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7239, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8419, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7723, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8011, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9923, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9452, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8400, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8282, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8084, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7187, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6219, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8618, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6471, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5575, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5136, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6077, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0198, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8350, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6771, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8841, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7108, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8070, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9164, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0196, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9300, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6503, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9090, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8105, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8374, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1085, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0297, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7375, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8263, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8004, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8772, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.3319, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4639, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0959, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9445, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7930, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6242, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8814, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9044, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8895, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9518, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9386, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8320, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9261, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7352, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8505, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9209, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9780, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8032, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9509, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6122, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9552, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9220, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0118, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7753, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5752, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6411, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0014, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9517, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8337, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8883, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8850, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9367, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3824, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7052, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0620, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1290, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0147, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9181, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9460, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9916, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7699, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6795, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1360, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9303, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8826, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9992, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7250, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0842, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9221, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7911, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7540, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6458, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6738, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7800, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8455, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9762, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0635, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0070, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0170, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7405, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.2582, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.3952, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9086, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8519, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9429, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9584, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0081, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1153, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8840, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8416, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5684, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9488, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7239, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9451, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6736, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7946, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9277, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8379, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7984, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8672, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9248, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8337, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9095, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0499, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7371, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6254, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9908, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0171, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7908, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8125, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8083, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7166, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1873, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1168, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8270, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7270, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6084, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5449, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6474, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9615, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9995, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8555, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5308, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9967, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7774, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8269, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7914, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1641, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7447, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8242, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2504, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8633, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8967, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0161, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7736, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1199, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7084, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6851, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7747, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7883, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7563, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7976, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7003, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8679, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1085, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9883, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9935, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0077, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8893, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7699, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7807, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9529, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0395, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8572, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8417, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8291, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0581, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9406, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0967, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3090, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2063, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0945, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8870, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6582, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7825, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7341, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9934, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1360, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9971, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8567, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9687, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7345, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8303, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0089, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7156, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8788, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1543, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1082, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0877, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5481, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7055, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8033, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8130, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7150, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8714, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8570, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6809, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8439, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8442, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0809, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8189, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8631, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1276, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9313, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0319, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7045, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8022, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6794, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7494, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8395, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6126, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9967, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9195, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0118, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9177, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1263, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9617, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9246, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8581, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8164, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9098, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8071, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8932, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9388, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8633, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7274, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9823, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0163, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0581, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6893, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5973, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7770, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7864, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4573, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.3600, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8099, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7513, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0968, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8012, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9095, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6127, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2372, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0690, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9495, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8633, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9992, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9546, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9784, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9846, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9201, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1266, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1762, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0760, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7965, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6225, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8966, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8956, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9299, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0286, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9071, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8953, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0021, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8396, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9682, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1207, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0621, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9237, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0021, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9227, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9559, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2622, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8570, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9185, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1020, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9353, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9124, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9851, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9517, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8170, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9307, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7388, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17456\\AppData\\Local\\Temp\\ipykernel_6272\\3417508707.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_y = torch.tensor(batch_y, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.003864533742875154, Validation Perplexity: 336.4918518066406\n"
     ]
    }
   ],
   "source": [
    "model_fixed_window = train_fixed_window(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance goal\n",
    "\n",
    "**Your submitted notebook must contain output demonstrating a validation perplexity of at most 350.** If you do not reach this perplexity after the first epoch, try training for a second epoch.\n",
    "\n",
    "‚ö†Ô∏è Computing the validation perplexity in one go (for the full validation set) will probably exhaust your computer‚Äôs memory and/or take a lot of time. If you run into this problem, do the computation at the minibatch level and aggregate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "To see whether your network is learning something, print or plot the loss and/or the perplexity on the training data. If the two values do not decrease during training, try to find the problem before wasting time (and electricity) on useless computation.\n",
    "\n",
    "Training and even evaluation will take some time ‚Äì on a CPU, you should expect several minutes per epoch, depending on hardware. Our reference implementation uses a GPU and runs in less than 30 seconds per epoch on [Colab](http://colab.research.google.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Recurrent neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement the recurrent neural network language model. Recall that an input to this model is a vector of word ids. Each integer is mapped to an embedding vector. The sequence of embedded vectors is then fed into an unrolled LSTM. At each position $i$ in the sequence, the hidden state of the LSTM at that position is sent through a linear transformation into a final softmax layer representing the probability distribution over the words at position $i+1$. In theory, the input vector could represent the complete training data; for practical reasons, however, we will truncate the input to some fixed value *bptt_len*. This length is called the **backpropagation-through-time horizon**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1: Vectorise the data (1&nbsp;point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous problem, your first task is to transform the data in the WikiText container into a vectorised form that can be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNVectorizer(object):\n",
    "    def __init__(self, bptt_len):\n",
    "        # backpropagation-through-time horizon\n",
    "        self.bptt_len = bptt_len\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # TODO: Replace the following line with your own code\n",
    "        num_sequences = len(data) // self.bptt_len\n",
    "\n",
    "        # Initialize X and Y tensors\n",
    "        X = torch.zeros((num_sequences, self.bptt_len), dtype=torch.long)\n",
    "        Y = torch.zeros((num_sequences, self.bptt_len), dtype=torch.long)\n",
    "\n",
    "        for i in range(num_sequences):\n",
    "            start_idx = i * self.bptt_len\n",
    "            end_idx = start_idx + self.bptt_len\n",
    "\n",
    "            # Populate X with the current sequence\n",
    "            X[i] = torch.tensor(data[start_idx:end_idx])\n",
    "\n",
    "            # Populate Y with the sequence offset by one position\n",
    "            Y[i] = torch.tensor(data[start_idx + 1:end_idx + 1] if end_idx < len(data) else data[start_idx + 1:] + [0])\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your vectoriser should meet the following specification:\n",
    "\n",
    "**__init__** (*self*, *bptt_len*)\n",
    "\n",
    "> Creates a new vectoriser. The parameter *bptt_len* specifies the backpropagation-through-time horizon.\n",
    "\n",
    "**__call__** (*self*, *data*)\n",
    "\n",
    "> Transforms a list of token indexes *data* into a pair of tensors $\\mathbf{X}$, $\\mathbf{Y}$ that can be used to train the recurrent neural language model. The rows of both tensors represent contiguous subsequences of token indexes of length *bptt_len*. Compared to the sequences in $\\mathbf{X}$, the corresponding sequences in $\\mathbf{Y}$ are shifted one position to the right. More precisely, if the $i$th row of $\\mathbf{X}$ is the sequence that starts at token position $j$, then the same row of $\\mathbf{Y}$ is the sequence that starts at position $j+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your implementation by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6837, 32]) torch.Size([6837, 32])\n"
     ]
    }
   ],
   "source": [
    "valid_x, valid_y = RNNVectorizer(32)(wikitext.valid)\n",
    "\n",
    "print(valid_x.size(), valid_y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = RNNVectorizer(32)(wikitext.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  1,  5,  0,  6,  7,  2,  8,  9, 10,  3, 11, 12,  9,\n",
       "        13, 14, 15, 16,  2, 17, 18, 19,  8, 20, 14, 21, 22, 23])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4,  1,  5,  0,  6,  7,  2,  8,  9, 10,  3, 11, 12,  9, 13,\n",
      "        14, 15, 16,  2, 17, 18, 19,  8, 20, 14, 21, 22, 23, 24])\n"
     ]
    }
   ],
   "source": [
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2: Implement the model (2&nbsp;points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to implement the recurrent neural network model based on the graphical specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_words, embedding_dim=50, hidden_dim=50):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_words, embedding_dim)\n",
    "        #nn.init.normal_(self.embedding.weight, mean=0, std=0.01)\n",
    "        #nn.init.xavier_normal_(self.embedding.weight)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim, n_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.LSTM(x)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should follow this specification:\n",
    "\n",
    "**__init__** (*self*, *n_words*, *embedding_dim* = 50, *hidden_dim* = 50)\n",
    "\n",
    "> Creates a new recurrent neural network language model based on an LSTM. The argument *n_words* is the number of words in the vocabulary. The arguments *embedding_dim* and *hidden_dim* specify the dimensionalities of the embedding layer and the LSTM hidden layer, respectively; their default value is 50.\n",
    "\n",
    "**forward** (*self*, *x*)\n",
    "\n",
    "> Computes the network output on an input batch *x*. The shape of *x* is $(B, H)$, where $B$ is the batch size and $H$ is the length of each input sequence. The shape of the output tensor is $(B, H, V)$, where $V$ is the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your code by instantiating the model and feeding it a batch of examples from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,     1, 32967, 32968,     1,     5,     0, 32967, 32968,    14,\n",
      "          407,    24,    18,  6254, 19903,   311,  1445, 19903,    14,    27,\n",
      "           28,  2577,    17,    10, 19903,   116,    18,  4930,  4122,  9612,\n",
      "           14,  4855])\n",
      "tensor([[-0.1278,  0.1540, -0.1164,  ..., -0.1109, -0.0335,  0.0066],\n",
      "        [-0.1460,  0.1070, -0.1035,  ..., -0.0228, -0.0723, -0.0985],\n",
      "        [-0.0490,  0.0793, -0.1817,  ..., -0.0906,  0.0745, -0.1262],\n",
      "        ...,\n",
      "        [-0.1130,  0.2508, -0.2141,  ..., -0.0333,  0.1696, -0.0889],\n",
      "        [-0.2479,  0.1773, -0.0813,  ...,  0.0078,  0.1846, -0.0999],\n",
      "        [-0.1508,  0.0464, -0.1393,  ..., -0.0494,  0.0285, -0.0111]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(valid_x[0])\n",
    "model = RNNModel(n_words=len(wikitext.vocab))\n",
    "output = model(valid_x[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3: Train the model (3&nbsp;points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop for the recurrent neural network model is essentially identical to the loop that you wrote for the feed-forward model. The only thing to note is that the cross-entropy loss function expects its input to be a two-dimensional tensor; you will therefore have to re-shape the output tensor from the LSTM as well as the gold-standard output tensor in a suitable way. The most efficient way to do so is to use the [`view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ...,    21,    22,    23],\n",
       "        [   24,     2,     3,  ...,    44,    26,    14],\n",
       "        [   47,    27,    18,  ...,    23,    18,    60],\n",
       "        ...,\n",
       "        [  349,     7,   428,  ...,   285, 23961,    27],\n",
       "        [  495,   490,   152,  ...,  4855,  2491,    16],\n",
       "        [   84,  9617,    27,  ...,   565,  1363,   152]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def get_batches(data, batch_size):\n",
    "    for i in range(0, len(data) - batch_size + 1, batch_size):\n",
    "        yield data[i:i + batch_size]\n",
    "\n",
    "\n",
    "def train_rnn(n_epochs=1, batch_size=100, lr=0.01):\n",
    "    # Initialize the model\n",
    "    model = RNNModel(n_words=len(wikitext.vocab))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Iterate over batches\n",
    "        for batch_x, batch_y in zip(get_batches(train_x, batch_size), get_batches(train_y, batch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "\n",
    "            # Reshape outputs and targets to fit CrossEntropyLoss expectations\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            batch_y = batch_y.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, batch_y) # why is better than F.cross_entropy\n",
    "            print(\"train_loss: \", loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss = total_loss * batch_size /len(train_x)\n",
    "\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in zip(get_batches(valid_x, batch_size), get_batches(valid_y, batch_size)):\n",
    "                batch_y = torch.tensor(batch_y, dtype=torch.long)\n",
    "                outputs = model(batch_x)\n",
    "\n",
    "                outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                batch_y = batch_y.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, batch_y) # why is better than F.cross_entropy\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "    valid_loss = total_loss * batch_size / len(valid_x)\n",
    "    perplexity = torch.exp(torch.tensor(valid_loss))\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Validation Perplexity: {perplexity}')\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Additional functions like 'create_batches' and 'calculate_perplexity' need to be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the training function:\n",
    "\n",
    "**train_rnn** (*n_epochs* = 1, *batch_size* = 1000, *bptt_len* = 32, *lr* = 0.01)\n",
    "\n",
    "> Trains a recurrent neural network language model on the WikiText data using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. The parameter *bptt_len* specifies the length of the backpropagation-through-time horizon, that is, the length of the input and output sequences. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*. After each epoch, prints the perplexity of the model on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model by running the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  10.419988632202148\n",
      "train_loss:  10.367169380187988\n",
      "train_loss:  10.286264419555664\n",
      "train_loss:  10.170522689819336\n",
      "train_loss:  9.953758239746094\n",
      "train_loss:  9.403770446777344\n",
      "train_loss:  9.008001327514648\n",
      "train_loss:  8.457756996154785\n",
      "train_loss:  7.991602897644043\n",
      "train_loss:  7.9494948387146\n",
      "train_loss:  7.667288303375244\n",
      "train_loss:  7.611982345581055\n",
      "train_loss:  7.6850152015686035\n",
      "train_loss:  7.884538650512695\n",
      "train_loss:  8.018348693847656\n",
      "train_loss:  7.731550693511963\n",
      "train_loss:  7.949888229370117\n",
      "train_loss:  7.852181911468506\n",
      "train_loss:  7.736701011657715\n",
      "train_loss:  7.6605072021484375\n",
      "train_loss:  7.801647186279297\n",
      "train_loss:  7.785073280334473\n",
      "train_loss:  7.921989917755127\n",
      "train_loss:  7.819475173950195\n",
      "train_loss:  7.5798115730285645\n",
      "train_loss:  7.608362197875977\n",
      "train_loss:  7.489190578460693\n",
      "train_loss:  7.577753067016602\n",
      "train_loss:  7.393876552581787\n",
      "train_loss:  7.679196357727051\n",
      "train_loss:  7.570830821990967\n",
      "train_loss:  7.502748966217041\n",
      "train_loss:  7.448215961456299\n",
      "train_loss:  7.391819477081299\n",
      "train_loss:  7.4197611808776855\n",
      "train_loss:  7.505812168121338\n",
      "train_loss:  7.438143730163574\n",
      "train_loss:  7.364780902862549\n",
      "train_loss:  7.391258716583252\n",
      "train_loss:  7.2889862060546875\n",
      "train_loss:  7.167884349822998\n",
      "train_loss:  7.4822678565979\n",
      "train_loss:  7.199624538421631\n",
      "train_loss:  7.3231892585754395\n",
      "train_loss:  7.308889865875244\n",
      "train_loss:  7.284546375274658\n",
      "train_loss:  7.105957508087158\n",
      "train_loss:  7.03689432144165\n",
      "train_loss:  7.298914909362793\n",
      "train_loss:  7.261226654052734\n",
      "train_loss:  7.275982856750488\n",
      "train_loss:  6.876966953277588\n",
      "train_loss:  7.060136795043945\n",
      "train_loss:  7.036041259765625\n",
      "train_loss:  6.988382339477539\n",
      "train_loss:  7.193396091461182\n",
      "train_loss:  7.115991592407227\n",
      "train_loss:  6.999664306640625\n",
      "train_loss:  6.977456092834473\n",
      "train_loss:  7.254197597503662\n",
      "train_loss:  7.263891220092773\n",
      "train_loss:  6.801819324493408\n",
      "train_loss:  6.9821696281433105\n",
      "train_loss:  6.997350692749023\n",
      "train_loss:  6.812192440032959\n",
      "train_loss:  6.738671779632568\n",
      "train_loss:  6.967776298522949\n",
      "train_loss:  7.274453639984131\n",
      "train_loss:  7.356220245361328\n",
      "train_loss:  6.91148567199707\n",
      "train_loss:  7.127105236053467\n",
      "train_loss:  7.221731185913086\n",
      "train_loss:  7.170289516448975\n",
      "train_loss:  6.986655235290527\n",
      "train_loss:  7.059589385986328\n",
      "train_loss:  7.004440784454346\n",
      "train_loss:  7.075344085693359\n",
      "train_loss:  7.033854961395264\n",
      "train_loss:  7.053605556488037\n",
      "train_loss:  7.070648670196533\n",
      "train_loss:  6.982964515686035\n",
      "train_loss:  7.1735663414001465\n",
      "train_loss:  7.016811370849609\n",
      "train_loss:  6.789794445037842\n",
      "train_loss:  6.892330169677734\n",
      "train_loss:  6.727994918823242\n",
      "train_loss:  6.697394847869873\n",
      "train_loss:  6.965182304382324\n",
      "train_loss:  6.954586982727051\n",
      "train_loss:  6.930846691131592\n",
      "train_loss:  7.12325382232666\n",
      "train_loss:  6.931978702545166\n",
      "train_loss:  7.064236640930176\n",
      "train_loss:  7.2131547927856445\n",
      "train_loss:  7.1705169677734375\n",
      "train_loss:  7.0532050132751465\n",
      "train_loss:  6.887988090515137\n",
      "train_loss:  6.893950939178467\n",
      "train_loss:  7.15535831451416\n",
      "train_loss:  7.134942054748535\n",
      "train_loss:  6.756700038909912\n",
      "train_loss:  6.809866905212402\n",
      "train_loss:  7.107168674468994\n",
      "train_loss:  7.0192646980285645\n",
      "train_loss:  7.003450870513916\n",
      "train_loss:  6.856982231140137\n",
      "train_loss:  7.123522758483887\n",
      "train_loss:  6.812530040740967\n",
      "train_loss:  6.9893012046813965\n",
      "train_loss:  6.791616916656494\n",
      "train_loss:  7.076864719390869\n",
      "train_loss:  6.9223504066467285\n",
      "train_loss:  6.938666820526123\n",
      "train_loss:  6.947383880615234\n",
      "train_loss:  6.808454513549805\n",
      "train_loss:  6.853069305419922\n",
      "train_loss:  6.761887073516846\n",
      "train_loss:  7.008264541625977\n",
      "train_loss:  7.078158378601074\n",
      "train_loss:  6.72396993637085\n",
      "train_loss:  6.784405708312988\n",
      "train_loss:  6.709366321563721\n",
      "train_loss:  6.746965408325195\n",
      "train_loss:  6.757263660430908\n",
      "train_loss:  7.034448146820068\n",
      "train_loss:  6.813373565673828\n",
      "train_loss:  6.8385329246521\n",
      "train_loss:  6.630463123321533\n",
      "train_loss:  6.5169830322265625\n",
      "train_loss:  6.513961315155029\n",
      "train_loss:  6.407642841339111\n",
      "train_loss:  6.541080951690674\n",
      "train_loss:  6.712810039520264\n",
      "train_loss:  6.758647918701172\n",
      "train_loss:  6.697707653045654\n",
      "train_loss:  6.609429359436035\n",
      "train_loss:  6.693815231323242\n",
      "train_loss:  6.664773941040039\n",
      "train_loss:  6.865555286407471\n",
      "train_loss:  6.7916107177734375\n",
      "train_loss:  6.85073709487915\n",
      "train_loss:  6.765722751617432\n",
      "train_loss:  6.6928558349609375\n",
      "train_loss:  7.041987419128418\n",
      "train_loss:  6.653073787689209\n",
      "train_loss:  6.515620231628418\n",
      "train_loss:  6.669774055480957\n",
      "train_loss:  6.947625160217285\n",
      "train_loss:  6.8401103019714355\n",
      "train_loss:  6.637669563293457\n",
      "train_loss:  6.605567455291748\n",
      "train_loss:  6.766406059265137\n",
      "train_loss:  6.8595967292785645\n",
      "train_loss:  6.841373920440674\n",
      "train_loss:  6.586802959442139\n",
      "train_loss:  6.341087818145752\n",
      "train_loss:  6.59036111831665\n",
      "train_loss:  6.799087047576904\n",
      "train_loss:  6.7046332359313965\n",
      "train_loss:  6.492794990539551\n",
      "train_loss:  6.685031890869141\n",
      "train_loss:  6.668757438659668\n",
      "train_loss:  6.553501129150391\n",
      "train_loss:  6.583975315093994\n",
      "train_loss:  6.749790191650391\n",
      "train_loss:  6.847226142883301\n",
      "train_loss:  6.618762969970703\n",
      "train_loss:  6.438860893249512\n",
      "train_loss:  6.421182632446289\n",
      "train_loss:  6.919112682342529\n",
      "train_loss:  7.141176700592041\n",
      "train_loss:  6.7895050048828125\n",
      "train_loss:  6.619223117828369\n",
      "train_loss:  6.519006252288818\n",
      "train_loss:  6.733595371246338\n",
      "train_loss:  6.7120442390441895\n",
      "train_loss:  6.598161220550537\n",
      "train_loss:  6.687190055847168\n",
      "train_loss:  6.744073867797852\n",
      "train_loss:  6.804673671722412\n",
      "train_loss:  6.784033298492432\n",
      "train_loss:  6.549854278564453\n",
      "train_loss:  6.692071437835693\n",
      "train_loss:  6.610475540161133\n",
      "train_loss:  6.613559722900391\n",
      "train_loss:  6.3131303787231445\n",
      "train_loss:  6.436700344085693\n",
      "train_loss:  6.509187698364258\n",
      "train_loss:  6.848888397216797\n",
      "train_loss:  6.404731273651123\n",
      "train_loss:  6.708320140838623\n",
      "train_loss:  6.692636489868164\n",
      "train_loss:  6.563953876495361\n",
      "train_loss:  6.322132110595703\n",
      "train_loss:  6.581140041351318\n",
      "train_loss:  6.509565353393555\n",
      "train_loss:  6.546990871429443\n",
      "train_loss:  6.4599928855896\n",
      "train_loss:  6.814329624176025\n",
      "train_loss:  6.612531661987305\n",
      "train_loss:  6.569589614868164\n",
      "train_loss:  6.368743419647217\n",
      "train_loss:  6.660515785217285\n",
      "train_loss:  6.542323589324951\n",
      "train_loss:  6.669158458709717\n",
      "train_loss:  6.817923545837402\n",
      "train_loss:  6.579622745513916\n",
      "train_loss:  6.6371564865112305\n",
      "train_loss:  6.3128228187561035\n",
      "train_loss:  6.527349472045898\n",
      "train_loss:  6.468017578125\n",
      "train_loss:  6.530138969421387\n",
      "train_loss:  6.466615200042725\n",
      "train_loss:  6.558223247528076\n",
      "train_loss:  7.059957981109619\n",
      "train_loss:  6.814903736114502\n",
      "train_loss:  6.783662796020508\n",
      "train_loss:  6.760807037353516\n",
      "train_loss:  6.704614639282227\n",
      "train_loss:  6.608570098876953\n",
      "train_loss:  6.380934238433838\n",
      "train_loss:  6.77225399017334\n",
      "train_loss:  6.608414173126221\n",
      "train_loss:  6.605870246887207\n",
      "train_loss:  6.530712127685547\n",
      "train_loss:  6.593048572540283\n",
      "train_loss:  6.45828104019165\n",
      "train_loss:  6.546521186828613\n",
      "train_loss:  6.506282806396484\n",
      "train_loss:  6.4634108543396\n",
      "train_loss:  6.225745677947998\n",
      "train_loss:  6.568215847015381\n",
      "train_loss:  6.346041202545166\n",
      "train_loss:  6.383302688598633\n",
      "train_loss:  6.109799385070801\n",
      "train_loss:  6.36849308013916\n",
      "train_loss:  6.647918701171875\n",
      "train_loss:  6.583340644836426\n",
      "train_loss:  6.436130523681641\n",
      "train_loss:  6.548061370849609\n",
      "train_loss:  6.577873706817627\n",
      "train_loss:  6.390017509460449\n",
      "train_loss:  6.492130279541016\n",
      "train_loss:  6.399394989013672\n",
      "train_loss:  6.48613166809082\n",
      "train_loss:  6.658505916595459\n",
      "train_loss:  6.522844314575195\n",
      "train_loss:  6.583524703979492\n",
      "train_loss:  6.500425815582275\n",
      "train_loss:  6.302742958068848\n",
      "train_loss:  6.366971969604492\n",
      "train_loss:  6.143660068511963\n",
      "train_loss:  6.173869609832764\n",
      "train_loss:  6.49262809753418\n",
      "train_loss:  6.4342732429504395\n",
      "train_loss:  6.78874397277832\n",
      "train_loss:  6.595749378204346\n",
      "train_loss:  6.577366828918457\n",
      "train_loss:  6.681669235229492\n",
      "train_loss:  6.3784942626953125\n",
      "train_loss:  6.612298011779785\n",
      "train_loss:  6.5472941398620605\n",
      "train_loss:  6.613170146942139\n",
      "train_loss:  6.599588394165039\n",
      "train_loss:  6.279602527618408\n",
      "train_loss:  6.260735988616943\n",
      "train_loss:  5.890499114990234\n",
      "train_loss:  6.03521728515625\n",
      "train_loss:  6.229753017425537\n",
      "train_loss:  6.662117958068848\n",
      "train_loss:  6.8826751708984375\n",
      "train_loss:  6.634101390838623\n",
      "train_loss:  6.637455940246582\n",
      "train_loss:  6.6992716789245605\n",
      "train_loss:  6.564770698547363\n",
      "train_loss:  6.542664051055908\n",
      "train_loss:  6.618635177612305\n",
      "train_loss:  6.164631366729736\n",
      "train_loss:  6.136114597320557\n",
      "train_loss:  6.488049507141113\n",
      "train_loss:  6.568756103515625\n",
      "train_loss:  6.5229082107543945\n",
      "train_loss:  6.505807876586914\n",
      "train_loss:  6.530496597290039\n",
      "train_loss:  6.4900736808776855\n",
      "train_loss:  6.445281505584717\n",
      "train_loss:  6.454052925109863\n",
      "train_loss:  6.426576137542725\n",
      "train_loss:  6.326340198516846\n",
      "train_loss:  6.317438125610352\n",
      "train_loss:  6.378322601318359\n",
      "train_loss:  6.1601433753967285\n",
      "train_loss:  6.275839805603027\n",
      "train_loss:  6.327480316162109\n",
      "train_loss:  6.5781097412109375\n",
      "train_loss:  6.583047866821289\n",
      "train_loss:  6.483270168304443\n",
      "train_loss:  6.5369553565979\n",
      "train_loss:  6.513395309448242\n",
      "train_loss:  6.542874336242676\n",
      "train_loss:  6.375843048095703\n",
      "train_loss:  6.695730686187744\n",
      "train_loss:  6.672004222869873\n",
      "train_loss:  6.479450225830078\n",
      "train_loss:  6.436438083648682\n",
      "train_loss:  6.330686092376709\n",
      "train_loss:  6.291375160217285\n",
      "train_loss:  6.2315802574157715\n",
      "train_loss:  6.249794960021973\n",
      "train_loss:  6.267434597015381\n",
      "train_loss:  6.730050563812256\n",
      "train_loss:  6.638113498687744\n",
      "train_loss:  6.715742588043213\n",
      "train_loss:  6.658343315124512\n",
      "train_loss:  6.3236918449401855\n",
      "train_loss:  6.678326606750488\n",
      "train_loss:  6.237619400024414\n",
      "train_loss:  6.294422626495361\n",
      "train_loss:  6.395572662353516\n",
      "train_loss:  6.322322845458984\n",
      "train_loss:  6.233627796173096\n",
      "train_loss:  6.0985026359558105\n",
      "train_loss:  6.2075629234313965\n",
      "train_loss:  6.151960849761963\n",
      "train_loss:  6.07028865814209\n",
      "train_loss:  6.253350257873535\n",
      "train_loss:  6.582284450531006\n",
      "train_loss:  6.565664291381836\n",
      "train_loss:  6.167433261871338\n",
      "train_loss:  6.42728328704834\n",
      "train_loss:  6.264003276824951\n",
      "train_loss:  6.419993877410889\n",
      "train_loss:  6.323763370513916\n",
      "train_loss:  6.251724720001221\n",
      "train_loss:  6.144956111907959\n",
      "train_loss:  6.2699761390686035\n",
      "train_loss:  6.178805351257324\n",
      "train_loss:  6.280151844024658\n",
      "train_loss:  6.504121780395508\n",
      "train_loss:  6.299043655395508\n",
      "train_loss:  6.097714900970459\n",
      "train_loss:  6.7997283935546875\n",
      "train_loss:  6.636332988739014\n",
      "train_loss:  6.437848091125488\n",
      "train_loss:  6.430550575256348\n",
      "train_loss:  6.2666730880737305\n",
      "train_loss:  6.522253036499023\n",
      "train_loss:  6.542299747467041\n",
      "train_loss:  6.4439568519592285\n",
      "train_loss:  6.471342086791992\n",
      "train_loss:  6.339500904083252\n",
      "train_loss:  6.615793228149414\n",
      "train_loss:  6.401617050170898\n",
      "train_loss:  6.7431769371032715\n",
      "train_loss:  6.203399181365967\n",
      "train_loss:  6.24333381652832\n",
      "train_loss:  6.328587532043457\n",
      "train_loss:  6.260234355926514\n",
      "train_loss:  6.359885215759277\n",
      "train_loss:  6.2753167152404785\n",
      "train_loss:  6.168213367462158\n",
      "train_loss:  6.2234930992126465\n",
      "train_loss:  6.2054443359375\n",
      "train_loss:  6.413376331329346\n",
      "train_loss:  6.529085159301758\n",
      "train_loss:  6.164524078369141\n",
      "train_loss:  6.0295233726501465\n",
      "train_loss:  6.127083778381348\n",
      "train_loss:  6.259731292724609\n",
      "train_loss:  6.664454936981201\n",
      "train_loss:  6.164674758911133\n",
      "train_loss:  6.301366806030273\n",
      "train_loss:  6.206638813018799\n",
      "train_loss:  6.2483391761779785\n",
      "train_loss:  6.442837715148926\n",
      "train_loss:  6.4434428215026855\n",
      "train_loss:  6.22242546081543\n",
      "train_loss:  6.192939281463623\n",
      "train_loss:  6.237456798553467\n",
      "train_loss:  6.084725379943848\n",
      "train_loss:  6.008094310760498\n",
      "train_loss:  6.306954383850098\n",
      "train_loss:  6.1554179191589355\n",
      "train_loss:  6.116346836090088\n",
      "train_loss:  6.199868202209473\n",
      "train_loss:  6.1465959548950195\n",
      "train_loss:  6.399178504943848\n",
      "train_loss:  6.260922908782959\n",
      "train_loss:  6.130321025848389\n",
      "train_loss:  6.341543674468994\n",
      "train_loss:  6.15655517578125\n",
      "train_loss:  6.308173656463623\n",
      "train_loss:  6.437819957733154\n",
      "train_loss:  6.417093276977539\n",
      "train_loss:  6.374871253967285\n",
      "train_loss:  6.068607330322266\n",
      "train_loss:  6.356470108032227\n",
      "train_loss:  6.179724216461182\n",
      "train_loss:  6.302045822143555\n",
      "train_loss:  6.484874725341797\n",
      "train_loss:  6.410426139831543\n",
      "train_loss:  6.163792610168457\n",
      "train_loss:  6.263318061828613\n",
      "train_loss:  6.2110161781311035\n",
      "train_loss:  6.410899639129639\n",
      "train_loss:  5.8714447021484375\n",
      "train_loss:  5.902434825897217\n",
      "train_loss:  6.508220672607422\n",
      "train_loss:  6.4086103439331055\n",
      "train_loss:  6.169696807861328\n",
      "train_loss:  6.01657247543335\n",
      "train_loss:  6.237370491027832\n",
      "train_loss:  6.332726001739502\n",
      "train_loss:  6.276737689971924\n",
      "train_loss:  6.436480522155762\n",
      "train_loss:  6.342255115509033\n",
      "train_loss:  6.16825008392334\n",
      "train_loss:  6.296693801879883\n",
      "train_loss:  6.172695159912109\n",
      "train_loss:  6.2526397705078125\n",
      "train_loss:  6.246271133422852\n",
      "train_loss:  6.395969390869141\n",
      "train_loss:  6.133098125457764\n",
      "train_loss:  6.306979179382324\n",
      "train_loss:  6.023289203643799\n",
      "train_loss:  6.389769077301025\n",
      "train_loss:  6.395817279815674\n",
      "train_loss:  6.406147003173828\n",
      "train_loss:  6.104721546173096\n",
      "train_loss:  6.133756637573242\n",
      "train_loss:  6.018316268920898\n",
      "train_loss:  6.451089859008789\n",
      "train_loss:  6.451400279998779\n",
      "train_loss:  6.3126983642578125\n",
      "train_loss:  6.253907680511475\n",
      "train_loss:  6.209325790405273\n",
      "train_loss:  6.269642353057861\n",
      "train_loss:  6.812442779541016\n",
      "train_loss:  6.14402961730957\n",
      "train_loss:  6.3628411293029785\n",
      "train_loss:  6.449777126312256\n",
      "train_loss:  6.319527626037598\n",
      "train_loss:  6.339768886566162\n",
      "train_loss:  6.327966690063477\n",
      "train_loss:  6.408298969268799\n",
      "train_loss:  6.052282333374023\n",
      "train_loss:  6.075459003448486\n",
      "train_loss:  6.7327728271484375\n",
      "train_loss:  6.365217208862305\n",
      "train_loss:  6.331798076629639\n",
      "train_loss:  6.332425117492676\n",
      "train_loss:  6.069485664367676\n",
      "train_loss:  6.464446544647217\n",
      "train_loss:  6.257554531097412\n",
      "train_loss:  6.0520219802856445\n",
      "train_loss:  6.112326145172119\n",
      "train_loss:  5.977477550506592\n",
      "train_loss:  6.037014961242676\n",
      "train_loss:  6.145692825317383\n",
      "train_loss:  6.222753524780273\n",
      "train_loss:  6.311468124389648\n",
      "train_loss:  6.449643135070801\n",
      "train_loss:  6.317989349365234\n",
      "train_loss:  6.347384452819824\n",
      "train_loss:  6.135005950927734\n",
      "train_loss:  5.802459716796875\n",
      "train_loss:  5.857287406921387\n",
      "train_loss:  6.208278179168701\n",
      "train_loss:  6.142129421234131\n",
      "train_loss:  6.150554656982422\n",
      "train_loss:  6.265078544616699\n",
      "train_loss:  6.260552883148193\n",
      "train_loss:  6.420592784881592\n",
      "train_loss:  6.188971042633057\n",
      "train_loss:  6.159033298492432\n",
      "train_loss:  5.881112098693848\n",
      "train_loss:  6.310760974884033\n",
      "train_loss:  5.999352931976318\n",
      "train_loss:  6.380283355712891\n",
      "train_loss:  6.044796943664551\n",
      "train_loss:  6.158799648284912\n",
      "train_loss:  6.3768391609191895\n",
      "train_loss:  6.177332878112793\n",
      "train_loss:  6.126469135284424\n",
      "train_loss:  6.193361759185791\n",
      "train_loss:  6.249265670776367\n",
      "train_loss:  6.1308183670043945\n",
      "train_loss:  6.219109535217285\n",
      "train_loss:  6.309937953948975\n",
      "train_loss:  6.0532307624816895\n",
      "train_loss:  5.966374397277832\n",
      "train_loss:  6.281095504760742\n",
      "train_loss:  6.282559394836426\n",
      "train_loss:  6.038450717926025\n",
      "train_loss:  6.123143196105957\n",
      "train_loss:  6.199136257171631\n",
      "train_loss:  6.0452656745910645\n",
      "train_loss:  6.631324291229248\n",
      "train_loss:  6.45247745513916\n",
      "train_loss:  6.097915172576904\n",
      "train_loss:  5.991182327270508\n",
      "train_loss:  5.94649600982666\n",
      "train_loss:  5.864015579223633\n",
      "train_loss:  5.974900722503662\n",
      "train_loss:  6.288812637329102\n",
      "train_loss:  6.247118949890137\n",
      "train_loss:  6.245270252227783\n",
      "train_loss:  5.912660121917725\n",
      "train_loss:  6.308859348297119\n",
      "train_loss:  6.093067169189453\n",
      "train_loss:  6.092504978179932\n",
      "train_loss:  6.089699745178223\n",
      "train_loss:  6.616269111633301\n",
      "train_loss:  6.083247661590576\n",
      "train_loss:  6.091146945953369\n",
      "train_loss:  6.797818660736084\n",
      "train_loss:  6.295558929443359\n",
      "train_loss:  6.178988456726074\n",
      "train_loss:  6.292880058288574\n",
      "train_loss:  6.150863170623779\n",
      "train_loss:  6.4139909744262695\n",
      "train_loss:  6.002868175506592\n",
      "train_loss:  5.921159267425537\n",
      "train_loss:  6.0100483894348145\n",
      "train_loss:  6.072004795074463\n",
      "train_loss:  6.035968780517578\n",
      "train_loss:  6.082746982574463\n",
      "train_loss:  5.968782424926758\n",
      "train_loss:  6.1496734619140625\n",
      "train_loss:  6.524153232574463\n",
      "train_loss:  6.389908313751221\n",
      "train_loss:  6.337551116943359\n",
      "train_loss:  6.406764984130859\n",
      "train_loss:  6.210272789001465\n",
      "train_loss:  6.0634331703186035\n",
      "train_loss:  6.072717189788818\n",
      "train_loss:  6.215310573577881\n",
      "train_loss:  6.359167575836182\n",
      "train_loss:  6.158869743347168\n",
      "train_loss:  6.1030731201171875\n",
      "train_loss:  6.092477321624756\n",
      "train_loss:  6.297230243682861\n",
      "train_loss:  6.182676315307617\n",
      "train_loss:  6.405627250671387\n",
      "train_loss:  6.62968635559082\n",
      "train_loss:  6.483906269073486\n",
      "train_loss:  6.329315185546875\n",
      "train_loss:  6.152408599853516\n",
      "train_loss:  5.902477264404297\n",
      "train_loss:  6.071918487548828\n",
      "train_loss:  6.027400970458984\n",
      "train_loss:  6.344542980194092\n",
      "train_loss:  6.388433933258057\n",
      "train_loss:  6.262551784515381\n",
      "train_loss:  6.18183708190918\n",
      "train_loss:  6.306707382202148\n",
      "train_loss:  6.099587440490723\n",
      "train_loss:  6.0924153327941895\n",
      "train_loss:  6.284936904907227\n",
      "train_loss:  5.960647106170654\n",
      "train_loss:  6.1462225914001465\n",
      "train_loss:  6.447359085083008\n",
      "train_loss:  6.361903667449951\n",
      "train_loss:  6.379889011383057\n",
      "train_loss:  5.910315036773682\n",
      "train_loss:  6.010613918304443\n",
      "train_loss:  6.103013515472412\n",
      "train_loss:  6.068341255187988\n",
      "train_loss:  6.0365471839904785\n",
      "train_loss:  6.23887825012207\n",
      "train_loss:  6.179071426391602\n",
      "train_loss:  5.941569805145264\n",
      "train_loss:  6.0919389724731445\n",
      "train_loss:  6.102346897125244\n",
      "train_loss:  6.356087684631348\n",
      "train_loss:  6.0781965255737305\n",
      "train_loss:  6.043188571929932\n",
      "train_loss:  6.4453301429748535\n",
      "train_loss:  6.284585475921631\n",
      "train_loss:  6.361512660980225\n",
      "train_loss:  5.96505880355835\n",
      "train_loss:  6.130479335784912\n",
      "train_loss:  5.992518901824951\n",
      "train_loss:  5.998888969421387\n",
      "train_loss:  6.093366622924805\n",
      "train_loss:  5.9478068351745605\n",
      "train_loss:  6.2653279304504395\n",
      "train_loss:  6.214345932006836\n",
      "train_loss:  6.282366752624512\n",
      "train_loss:  6.162041187286377\n",
      "train_loss:  6.452165126800537\n",
      "train_loss:  6.156924247741699\n",
      "train_loss:  6.168223857879639\n",
      "train_loss:  6.164675712585449\n",
      "train_loss:  6.103992938995361\n",
      "train_loss:  6.1550984382629395\n",
      "train_loss:  6.0294060707092285\n",
      "train_loss:  6.146295070648193\n",
      "train_loss:  6.171147346496582\n",
      "train_loss:  6.08171272277832\n",
      "train_loss:  6.064406871795654\n",
      "train_loss:  6.285064697265625\n",
      "train_loss:  6.303427696228027\n",
      "train_loss:  6.342048168182373\n",
      "train_loss:  6.0577192306518555\n",
      "train_loss:  5.834153652191162\n",
      "train_loss:  6.091823577880859\n",
      "train_loss:  6.005427837371826\n",
      "train_loss:  5.755967617034912\n",
      "train_loss:  5.654153823852539\n",
      "train_loss:  6.056369781494141\n",
      "train_loss:  5.980569839477539\n",
      "train_loss:  6.264556407928467\n",
      "train_loss:  6.042516708374023\n",
      "train_loss:  6.1120219230651855\n",
      "train_loss:  5.84321403503418\n",
      "train_loss:  6.525089740753174\n",
      "train_loss:  6.332151889801025\n",
      "train_loss:  6.228957653045654\n",
      "train_loss:  6.2183685302734375\n",
      "train_loss:  6.2357587814331055\n",
      "train_loss:  6.138546943664551\n",
      "train_loss:  6.16294002532959\n",
      "train_loss:  6.2383832931518555\n",
      "train_loss:  6.18861198425293\n",
      "train_loss:  6.2942914962768555\n",
      "train_loss:  6.363571643829346\n",
      "train_loss:  6.264218330383301\n",
      "train_loss:  6.071171283721924\n",
      "train_loss:  5.746812343597412\n",
      "train_loss:  6.160402297973633\n",
      "train_loss:  6.0875115394592285\n",
      "train_loss:  6.107491493225098\n",
      "train_loss:  6.223398208618164\n",
      "train_loss:  6.106069564819336\n",
      "train_loss:  6.0933661460876465\n",
      "train_loss:  6.226596832275391\n",
      "train_loss:  6.0321879386901855\n",
      "train_loss:  6.207717418670654\n",
      "train_loss:  6.4223856925964355\n",
      "train_loss:  6.2299418449401855\n",
      "train_loss:  6.090628623962402\n",
      "train_loss:  6.188910007476807\n",
      "train_loss:  6.13408088684082\n",
      "train_loss:  6.205405235290527\n",
      "train_loss:  6.450842380523682\n",
      "train_loss:  6.015076160430908\n",
      "train_loss:  6.098095893859863\n",
      "train_loss:  6.368800163269043\n",
      "train_loss:  6.187419414520264\n",
      "train_loss:  6.1914167404174805\n",
      "train_loss:  6.2103376388549805\n",
      "train_loss:  6.289099216461182\n",
      "train_loss:  6.010129928588867\n",
      "train_loss:  6.111238956451416\n",
      "train_loss:  5.9083251953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17456\\AppData\\Local\\Temp\\ipykernel_26224\\1643080265.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_y = torch.tensor(batch_y, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  6.030330181121826\n",
      "train_loss:  5.861627578735352\n",
      "train_loss:  6.013350963592529\n",
      "train_loss:  6.127972602844238\n",
      "train_loss:  6.037977695465088\n",
      "train_loss:  5.711465835571289\n",
      "train_loss:  5.860497951507568\n",
      "train_loss:  5.970273017883301\n",
      "train_loss:  5.897275447845459\n",
      "train_loss:  5.882673263549805\n",
      "train_loss:  5.873165130615234\n",
      "train_loss:  5.982775688171387\n",
      "train_loss:  6.099665641784668\n",
      "train_loss:  6.2073974609375\n",
      "train_loss:  6.163914680480957\n",
      "train_loss:  5.934309482574463\n",
      "train_loss:  5.894278526306152\n",
      "train_loss:  5.959204196929932\n",
      "train_loss:  5.847671031951904\n",
      "train_loss:  5.79086446762085\n",
      "train_loss:  5.914404392242432\n",
      "train_loss:  5.951840400695801\n",
      "train_loss:  6.118196487426758\n",
      "train_loss:  6.045746326446533\n",
      "train_loss:  5.862207412719727\n",
      "train_loss:  5.947079658508301\n",
      "train_loss:  5.967791557312012\n",
      "train_loss:  6.057098388671875\n",
      "train_loss:  5.771705150604248\n",
      "train_loss:  5.978606700897217\n",
      "train_loss:  6.027604579925537\n",
      "train_loss:  5.879590034484863\n",
      "train_loss:  5.90854549407959\n",
      "train_loss:  5.961689472198486\n",
      "train_loss:  5.900515556335449\n",
      "train_loss:  6.033336162567139\n",
      "train_loss:  5.961304187774658\n",
      "train_loss:  5.903176784515381\n",
      "train_loss:  5.914857387542725\n",
      "train_loss:  5.857332229614258\n",
      "train_loss:  5.762650966644287\n",
      "train_loss:  5.978472709655762\n",
      "train_loss:  5.874600410461426\n",
      "train_loss:  6.0494384765625\n",
      "train_loss:  5.795501708984375\n",
      "train_loss:  5.9637603759765625\n",
      "train_loss:  5.86557149887085\n",
      "train_loss:  5.637340068817139\n",
      "train_loss:  6.0229973793029785\n",
      "train_loss:  6.097266674041748\n",
      "train_loss:  6.1243815422058105\n",
      "train_loss:  5.698106288909912\n",
      "train_loss:  5.870151519775391\n",
      "train_loss:  5.8518548011779785\n",
      "train_loss:  5.779090404510498\n",
      "train_loss:  5.887078285217285\n",
      "train_loss:  5.849242687225342\n",
      "train_loss:  5.6840643882751465\n",
      "train_loss:  5.701350212097168\n",
      "train_loss:  5.9969682693481445\n",
      "train_loss:  5.986070156097412\n",
      "train_loss:  5.621668815612793\n",
      "train_loss:  5.882165431976318\n",
      "train_loss:  5.724642276763916\n",
      "train_loss:  5.707237720489502\n",
      "train_loss:  5.532650947570801\n",
      "train_loss:  5.581976890563965\n",
      "train_loss:  6.101726531982422\n",
      "train_loss:  6.10856819152832\n",
      "train_loss:  5.754018783569336\n",
      "train_loss:  5.957037925720215\n",
      "train_loss:  5.8893280029296875\n",
      "train_loss:  6.0227484703063965\n",
      "train_loss:  5.827948570251465\n",
      "train_loss:  5.992220878601074\n",
      "train_loss:  5.790865898132324\n",
      "train_loss:  5.914274215698242\n",
      "train_loss:  6.002407073974609\n",
      "train_loss:  5.8747124671936035\n",
      "train_loss:  6.045497894287109\n",
      "train_loss:  5.793190956115723\n",
      "train_loss:  6.128610610961914\n",
      "train_loss:  5.978498935699463\n",
      "train_loss:  5.809886455535889\n",
      "train_loss:  5.978506088256836\n",
      "train_loss:  5.888787269592285\n",
      "train_loss:  5.854536056518555\n",
      "train_loss:  6.01338005065918\n",
      "train_loss:  5.871135711669922\n",
      "train_loss:  5.837836742401123\n",
      "train_loss:  6.059845447540283\n",
      "train_loss:  5.877740383148193\n",
      "train_loss:  6.069246768951416\n",
      "train_loss:  6.133367538452148\n",
      "train_loss:  6.157953262329102\n",
      "train_loss:  6.013351917266846\n",
      "train_loss:  5.833569526672363\n",
      "train_loss:  5.777647495269775\n",
      "train_loss:  5.899407863616943\n",
      "train_loss:  5.964737415313721\n",
      "train_loss:  5.769920825958252\n",
      "train_loss:  5.8483381271362305\n",
      "train_loss:  6.225508213043213\n",
      "train_loss:  5.990214824676514\n",
      "train_loss:  6.036128520965576\n",
      "train_loss:  5.965014457702637\n",
      "train_loss:  6.3598737716674805\n",
      "train_loss:  5.9064483642578125\n",
      "train_loss:  5.983046054840088\n",
      "train_loss:  5.727850914001465\n",
      "train_loss:  6.143962383270264\n",
      "train_loss:  6.0204033851623535\n",
      "train_loss:  6.119826793670654\n",
      "train_loss:  5.9087138175964355\n",
      "train_loss:  5.870941162109375\n",
      "train_loss:  5.976007461547852\n",
      "train_loss:  5.978113174438477\n",
      "train_loss:  5.9855546951293945\n",
      "train_loss:  5.922374725341797\n",
      "train_loss:  5.788750648498535\n",
      "train_loss:  5.877798080444336\n",
      "train_loss:  5.811471939086914\n",
      "train_loss:  5.711093902587891\n",
      "train_loss:  5.936426162719727\n",
      "train_loss:  6.238758087158203\n",
      "train_loss:  5.918692588806152\n",
      "train_loss:  5.980976581573486\n",
      "train_loss:  5.8097453117370605\n",
      "train_loss:  5.7319655418396\n",
      "train_loss:  5.672018527984619\n",
      "train_loss:  5.598979949951172\n",
      "train_loss:  5.714526176452637\n",
      "train_loss:  5.8224663734436035\n",
      "train_loss:  5.91476583480835\n",
      "train_loss:  5.848791122436523\n",
      "train_loss:  5.813075542449951\n",
      "train_loss:  5.900399684906006\n",
      "train_loss:  5.664621353149414\n",
      "train_loss:  5.883479595184326\n",
      "train_loss:  5.976024150848389\n",
      "train_loss:  6.031498908996582\n",
      "train_loss:  5.966330051422119\n",
      "train_loss:  5.834674835205078\n",
      "train_loss:  6.133834362030029\n",
      "train_loss:  5.8226776123046875\n",
      "train_loss:  5.657209873199463\n",
      "train_loss:  5.874250411987305\n",
      "train_loss:  5.990795135498047\n",
      "train_loss:  5.971286773681641\n",
      "train_loss:  5.756683826446533\n",
      "train_loss:  5.668100357055664\n",
      "train_loss:  5.853155612945557\n",
      "train_loss:  5.9501633644104\n",
      "train_loss:  5.968911647796631\n",
      "train_loss:  5.78177547454834\n",
      "train_loss:  5.461331844329834\n",
      "train_loss:  5.691629409790039\n",
      "train_loss:  5.829930305480957\n",
      "train_loss:  5.4359049797058105\n",
      "train_loss:  5.8053717613220215\n",
      "train_loss:  5.8929524421691895\n",
      "train_loss:  5.67018985748291\n",
      "train_loss:  5.648665904998779\n",
      "train_loss:  5.716444492340088\n",
      "train_loss:  5.920476913452148\n",
      "train_loss:  6.053116321563721\n",
      "train_loss:  5.820919036865234\n",
      "train_loss:  5.620303153991699\n",
      "train_loss:  5.614718437194824\n",
      "train_loss:  6.037816047668457\n",
      "train_loss:  6.268531322479248\n",
      "train_loss:  5.911425590515137\n",
      "train_loss:  5.802751064300537\n",
      "train_loss:  5.656429290771484\n",
      "train_loss:  5.810250282287598\n",
      "train_loss:  5.832368850708008\n",
      "train_loss:  5.775039196014404\n",
      "train_loss:  5.907520294189453\n",
      "train_loss:  6.0302605628967285\n",
      "train_loss:  5.920663356781006\n",
      "train_loss:  5.956085205078125\n",
      "train_loss:  5.7462849617004395\n",
      "train_loss:  5.952241897583008\n",
      "train_loss:  5.85658073425293\n",
      "train_loss:  5.861197471618652\n",
      "train_loss:  5.466121673583984\n",
      "train_loss:  5.584697723388672\n",
      "train_loss:  5.6663923263549805\n",
      "train_loss:  6.034430980682373\n",
      "train_loss:  5.592257022857666\n",
      "train_loss:  5.8898468017578125\n",
      "train_loss:  5.968942642211914\n",
      "train_loss:  5.835593223571777\n",
      "train_loss:  5.598452091217041\n",
      "train_loss:  5.87019157409668\n",
      "train_loss:  5.728374004364014\n",
      "train_loss:  5.735142230987549\n",
      "train_loss:  5.7098774909973145\n",
      "train_loss:  6.03518009185791\n",
      "train_loss:  5.908880233764648\n",
      "train_loss:  5.902116775512695\n",
      "train_loss:  5.689895153045654\n",
      "train_loss:  5.839193344116211\n",
      "train_loss:  5.762170791625977\n",
      "train_loss:  5.9482855796813965\n",
      "train_loss:  6.032780647277832\n",
      "train_loss:  5.849844455718994\n",
      "train_loss:  5.942543983459473\n",
      "train_loss:  5.731255531311035\n",
      "train_loss:  5.621215343475342\n",
      "train_loss:  5.743887424468994\n",
      "train_loss:  5.741424083709717\n",
      "train_loss:  5.635393142700195\n",
      "train_loss:  5.862171649932861\n",
      "train_loss:  6.2502241134643555\n",
      "train_loss:  5.9758429527282715\n",
      "train_loss:  5.971713066101074\n",
      "train_loss:  6.090802192687988\n",
      "train_loss:  5.9086527824401855\n",
      "train_loss:  5.904412746429443\n",
      "train_loss:  5.6032514572143555\n",
      "train_loss:  5.944035053253174\n",
      "train_loss:  5.790543079376221\n",
      "train_loss:  5.800776481628418\n",
      "train_loss:  5.7789483070373535\n",
      "train_loss:  5.901656627655029\n",
      "train_loss:  5.704997539520264\n",
      "train_loss:  5.897005558013916\n",
      "train_loss:  5.78038215637207\n",
      "train_loss:  5.844100475311279\n",
      "train_loss:  5.568670749664307\n",
      "train_loss:  5.799597263336182\n",
      "train_loss:  5.609411239624023\n",
      "train_loss:  5.698300361633301\n",
      "train_loss:  5.3624677658081055\n",
      "train_loss:  5.673442363739014\n",
      "train_loss:  5.947173595428467\n",
      "train_loss:  5.805566787719727\n",
      "train_loss:  5.796990394592285\n",
      "train_loss:  5.937524318695068\n",
      "train_loss:  5.830361843109131\n",
      "train_loss:  5.62932014465332\n",
      "train_loss:  5.769631385803223\n",
      "train_loss:  5.669722080230713\n",
      "train_loss:  5.81520938873291\n",
      "train_loss:  6.020001411437988\n",
      "train_loss:  5.883725643157959\n",
      "train_loss:  5.6913161277771\n",
      "train_loss:  5.771803379058838\n",
      "train_loss:  5.656594753265381\n",
      "train_loss:  5.7451372146606445\n",
      "train_loss:  5.530098915100098\n",
      "train_loss:  5.524783134460449\n",
      "train_loss:  5.765133857727051\n",
      "train_loss:  5.5989203453063965\n",
      "train_loss:  6.044551372528076\n",
      "train_loss:  5.895638942718506\n",
      "train_loss:  5.880839824676514\n",
      "train_loss:  6.029224395751953\n",
      "train_loss:  5.639582633972168\n",
      "train_loss:  5.9319024085998535\n",
      "train_loss:  5.859562873840332\n",
      "train_loss:  5.801192760467529\n",
      "train_loss:  5.96453857421875\n",
      "train_loss:  5.510403633117676\n",
      "train_loss:  5.610586643218994\n",
      "train_loss:  5.334472179412842\n",
      "train_loss:  5.3921799659729\n",
      "train_loss:  5.518729209899902\n",
      "train_loss:  5.9209723472595215\n",
      "train_loss:  6.0009565353393555\n",
      "train_loss:  5.915237903594971\n",
      "train_loss:  5.876407623291016\n",
      "train_loss:  6.015019416809082\n",
      "train_loss:  5.896024227142334\n",
      "train_loss:  5.841977119445801\n",
      "train_loss:  5.71833610534668\n",
      "train_loss:  5.502391338348389\n",
      "train_loss:  5.558610916137695\n",
      "train_loss:  5.878242015838623\n",
      "train_loss:  5.883896350860596\n",
      "train_loss:  5.890657901763916\n",
      "train_loss:  5.824373245239258\n",
      "train_loss:  5.907773017883301\n",
      "train_loss:  5.839390277862549\n",
      "train_loss:  5.749481201171875\n",
      "train_loss:  5.786708354949951\n",
      "train_loss:  5.693589687347412\n",
      "train_loss:  5.648106098175049\n",
      "train_loss:  5.667352199554443\n",
      "train_loss:  5.715554714202881\n",
      "train_loss:  5.4752702713012695\n",
      "train_loss:  5.542203903198242\n",
      "train_loss:  5.730828285217285\n",
      "train_loss:  5.860198497772217\n",
      "train_loss:  5.811600208282471\n",
      "train_loss:  5.838611602783203\n",
      "train_loss:  5.935397148132324\n",
      "train_loss:  5.8923234939575195\n",
      "train_loss:  5.903463840484619\n",
      "train_loss:  5.629472732543945\n",
      "train_loss:  5.916803359985352\n",
      "train_loss:  5.920197010040283\n",
      "train_loss:  5.755237579345703\n",
      "train_loss:  5.844452381134033\n",
      "train_loss:  5.455388069152832\n",
      "train_loss:  5.568346977233887\n",
      "train_loss:  5.653726577758789\n",
      "train_loss:  5.612833023071289\n",
      "train_loss:  5.56564474105835\n",
      "train_loss:  5.924152851104736\n",
      "train_loss:  5.8871541023254395\n",
      "train_loss:  5.95071268081665\n",
      "train_loss:  5.901247024536133\n",
      "train_loss:  5.632504940032959\n",
      "train_loss:  6.067363262176514\n",
      "train_loss:  5.650568008422852\n",
      "train_loss:  5.6275482177734375\n",
      "train_loss:  5.862697124481201\n",
      "train_loss:  5.762581825256348\n",
      "train_loss:  5.7245988845825195\n",
      "train_loss:  5.5378851890563965\n",
      "train_loss:  5.651923179626465\n",
      "train_loss:  5.608372688293457\n",
      "train_loss:  5.546519756317139\n",
      "train_loss:  5.5624189376831055\n",
      "train_loss:  6.028827667236328\n",
      "train_loss:  5.9564642906188965\n",
      "train_loss:  5.611590385437012\n",
      "train_loss:  5.693679332733154\n",
      "train_loss:  5.516773223876953\n",
      "train_loss:  5.784801006317139\n",
      "train_loss:  5.728323459625244\n",
      "train_loss:  5.6480255126953125\n",
      "train_loss:  5.476215362548828\n",
      "train_loss:  5.673126697540283\n",
      "train_loss:  5.72784423828125\n",
      "train_loss:  5.7021803855896\n",
      "train_loss:  5.718811511993408\n",
      "train_loss:  5.667074203491211\n",
      "train_loss:  5.44039249420166\n",
      "train_loss:  6.1185150146484375\n",
      "train_loss:  5.9404215812683105\n",
      "train_loss:  5.799830436706543\n",
      "train_loss:  5.8663811683654785\n",
      "train_loss:  5.659882068634033\n",
      "train_loss:  5.937102794647217\n",
      "train_loss:  5.96735954284668\n",
      "train_loss:  5.876070022583008\n",
      "train_loss:  5.850657939910889\n",
      "train_loss:  5.682137966156006\n",
      "train_loss:  5.900243759155273\n",
      "train_loss:  5.749409198760986\n",
      "train_loss:  6.058328151702881\n",
      "train_loss:  5.660999774932861\n",
      "train_loss:  5.726398944854736\n",
      "train_loss:  5.7962260246276855\n",
      "train_loss:  5.609486103057861\n",
      "train_loss:  5.833358287811279\n",
      "train_loss:  5.761779308319092\n",
      "train_loss:  5.65941047668457\n",
      "train_loss:  5.67348575592041\n",
      "train_loss:  5.640080451965332\n",
      "train_loss:  5.793292045593262\n",
      "train_loss:  5.999392509460449\n",
      "train_loss:  5.616900444030762\n",
      "train_loss:  5.478668689727783\n",
      "train_loss:  5.600438117980957\n",
      "train_loss:  5.760937690734863\n",
      "train_loss:  5.996770858764648\n",
      "train_loss:  5.605085849761963\n",
      "train_loss:  5.73768424987793\n",
      "train_loss:  5.6712260246276855\n",
      "train_loss:  5.696157455444336\n",
      "train_loss:  5.8657073974609375\n",
      "train_loss:  5.881521224975586\n",
      "train_loss:  5.741847991943359\n",
      "train_loss:  5.6828813552856445\n",
      "train_loss:  5.72163200378418\n",
      "train_loss:  5.595134735107422\n",
      "train_loss:  5.543631553649902\n",
      "train_loss:  5.750994682312012\n",
      "train_loss:  5.472462177276611\n",
      "train_loss:  5.463123321533203\n",
      "train_loss:  5.4682207107543945\n",
      "train_loss:  5.565216064453125\n",
      "train_loss:  5.89005184173584\n",
      "train_loss:  5.726930141448975\n",
      "train_loss:  5.560811996459961\n",
      "train_loss:  5.804922580718994\n",
      "train_loss:  5.55221700668335\n",
      "train_loss:  5.714150428771973\n",
      "train_loss:  5.839269638061523\n",
      "train_loss:  5.8788580894470215\n",
      "train_loss:  5.807229042053223\n",
      "train_loss:  5.4902119636535645\n",
      "train_loss:  5.753358364105225\n",
      "train_loss:  5.665028095245361\n",
      "train_loss:  5.72476863861084\n",
      "train_loss:  5.98822021484375\n",
      "train_loss:  5.908554553985596\n",
      "train_loss:  5.608490467071533\n",
      "train_loss:  5.693670272827148\n",
      "train_loss:  5.673555850982666\n",
      "train_loss:  5.774588584899902\n",
      "train_loss:  5.153001308441162\n",
      "train_loss:  5.277564525604248\n",
      "train_loss:  5.977717399597168\n",
      "train_loss:  5.871321201324463\n",
      "train_loss:  5.688694000244141\n",
      "train_loss:  5.481355667114258\n",
      "train_loss:  5.745570659637451\n",
      "train_loss:  5.769993782043457\n",
      "train_loss:  5.756290435791016\n",
      "train_loss:  5.879074573516846\n",
      "train_loss:  5.828160285949707\n",
      "train_loss:  5.702775478363037\n",
      "train_loss:  5.750371932983398\n",
      "train_loss:  5.608829975128174\n",
      "train_loss:  5.675452709197998\n",
      "train_loss:  5.728890895843506\n",
      "train_loss:  5.822793960571289\n",
      "train_loss:  5.640348434448242\n",
      "train_loss:  5.8475565910339355\n",
      "train_loss:  5.433212757110596\n",
      "train_loss:  5.867011547088623\n",
      "train_loss:  5.848789215087891\n",
      "train_loss:  5.895757675170898\n",
      "train_loss:  5.608376979827881\n",
      "train_loss:  5.5149712562561035\n",
      "train_loss:  5.484784126281738\n",
      "train_loss:  5.860598087310791\n",
      "train_loss:  5.851883411407471\n",
      "train_loss:  5.702406406402588\n",
      "train_loss:  5.769428730010986\n",
      "train_loss:  5.750791072845459\n",
      "train_loss:  5.80098819732666\n",
      "train_loss:  6.2151970863342285\n",
      "train_loss:  5.585692882537842\n",
      "train_loss:  5.9345622062683105\n",
      "train_loss:  6.0052995681762695\n",
      "train_loss:  5.874249458312988\n",
      "train_loss:  5.801762580871582\n",
      "train_loss:  5.764487266540527\n",
      "train_loss:  5.851327896118164\n",
      "train_loss:  5.5985612869262695\n",
      "train_loss:  5.521261215209961\n",
      "train_loss:  6.002711296081543\n",
      "train_loss:  5.806262016296387\n",
      "train_loss:  5.772777557373047\n",
      "train_loss:  5.874882698059082\n",
      "train_loss:  5.523785591125488\n",
      "train_loss:  5.937175750732422\n",
      "train_loss:  5.806640625\n",
      "train_loss:  5.6655073165893555\n",
      "train_loss:  5.657819747924805\n",
      "train_loss:  5.513942718505859\n",
      "train_loss:  5.5136871337890625\n",
      "train_loss:  5.653214931488037\n",
      "train_loss:  5.685494899749756\n",
      "train_loss:  5.802025318145752\n",
      "train_loss:  5.907313823699951\n",
      "train_loss:  5.814201831817627\n",
      "train_loss:  5.879289150238037\n",
      "train_loss:  5.604017734527588\n",
      "train_loss:  5.129631519317627\n",
      "train_loss:  5.257758617401123\n",
      "train_loss:  5.812607288360596\n",
      "train_loss:  5.740092754364014\n",
      "train_loss:  5.820735931396484\n",
      "train_loss:  5.833826065063477\n",
      "train_loss:  5.88510274887085\n",
      "train_loss:  5.940708637237549\n",
      "train_loss:  5.712345123291016\n",
      "train_loss:  5.738229274749756\n",
      "train_loss:  5.435076713562012\n",
      "train_loss:  5.862182140350342\n",
      "train_loss:  5.566072940826416\n",
      "train_loss:  5.793585300445557\n",
      "train_loss:  5.580411911010742\n",
      "train_loss:  5.718878269195557\n",
      "train_loss:  5.833877086639404\n",
      "train_loss:  5.679323196411133\n",
      "train_loss:  5.672661781311035\n",
      "train_loss:  5.765688419342041\n",
      "train_loss:  5.819261074066162\n",
      "train_loss:  5.6883039474487305\n",
      "train_loss:  5.669927597045898\n",
      "train_loss:  5.8684515953063965\n",
      "train_loss:  5.576852798461914\n",
      "train_loss:  5.4416351318359375\n",
      "train_loss:  5.84104061126709\n",
      "train_loss:  5.8248090744018555\n",
      "train_loss:  5.625579357147217\n",
      "train_loss:  5.674106597900391\n",
      "train_loss:  5.644003391265869\n",
      "train_loss:  5.549213886260986\n",
      "train_loss:  6.051161289215088\n",
      "train_loss:  5.964585781097412\n",
      "train_loss:  5.676032543182373\n",
      "train_loss:  5.5204010009765625\n",
      "train_loss:  5.463923454284668\n",
      "train_loss:  5.425946235656738\n",
      "train_loss:  5.5218071937561035\n",
      "train_loss:  5.8569464683532715\n",
      "train_loss:  5.843268394470215\n",
      "train_loss:  5.744166374206543\n",
      "train_loss:  5.366969108581543\n",
      "train_loss:  5.796327114105225\n",
      "train_loss:  5.582244396209717\n",
      "train_loss:  5.682610511779785\n",
      "train_loss:  5.6558918952941895\n",
      "train_loss:  6.0272216796875\n",
      "train_loss:  5.619849681854248\n",
      "train_loss:  5.665665149688721\n",
      "train_loss:  6.102472305297852\n",
      "train_loss:  5.7131171226501465\n",
      "train_loss:  5.762271881103516\n",
      "train_loss:  5.8817877769470215\n",
      "train_loss:  5.628094673156738\n",
      "train_loss:  5.942892551422119\n",
      "train_loss:  5.497241020202637\n",
      "train_loss:  5.503847122192383\n",
      "train_loss:  5.583607196807861\n",
      "train_loss:  5.623656749725342\n",
      "train_loss:  5.579830169677734\n",
      "train_loss:  5.6017351150512695\n",
      "train_loss:  5.5587005615234375\n",
      "train_loss:  5.66644287109375\n",
      "train_loss:  5.978822708129883\n",
      "train_loss:  5.852218151092529\n",
      "train_loss:  5.8259100914001465\n",
      "train_loss:  5.87992000579834\n",
      "train_loss:  5.710198879241943\n",
      "train_loss:  5.6107177734375\n",
      "train_loss:  5.581561088562012\n",
      "train_loss:  5.744534969329834\n",
      "train_loss:  5.926774501800537\n",
      "train_loss:  5.702131748199463\n",
      "train_loss:  5.686687469482422\n",
      "train_loss:  5.703150749206543\n",
      "train_loss:  5.920757293701172\n",
      "train_loss:  5.7885212898254395\n",
      "train_loss:  5.958781719207764\n",
      "train_loss:  6.19183349609375\n",
      "train_loss:  6.0472493171691895\n",
      "train_loss:  5.941972732543945\n",
      "train_loss:  5.736672878265381\n",
      "train_loss:  5.476668834686279\n",
      "train_loss:  5.665520668029785\n",
      "train_loss:  5.599441051483154\n",
      "train_loss:  5.852689743041992\n",
      "train_loss:  5.9782490730285645\n",
      "train_loss:  5.830560207366943\n",
      "train_loss:  5.662956714630127\n",
      "train_loss:  5.835402011871338\n",
      "train_loss:  5.55972146987915\n",
      "train_loss:  5.628757476806641\n",
      "train_loss:  5.821854114532471\n",
      "train_loss:  5.552237510681152\n",
      "train_loss:  5.739355564117432\n",
      "train_loss:  6.021275043487549\n",
      "train_loss:  5.939799785614014\n",
      "train_loss:  5.969443321228027\n",
      "train_loss:  5.4026665687561035\n",
      "train_loss:  5.5500922203063965\n",
      "train_loss:  5.663593769073486\n",
      "train_loss:  5.611865043640137\n",
      "train_loss:  5.586669921875\n",
      "train_loss:  5.762231349945068\n",
      "train_loss:  5.751223087310791\n",
      "train_loss:  5.524163722991943\n",
      "train_loss:  5.667886734008789\n",
      "train_loss:  5.6555585861206055\n",
      "train_loss:  5.929221153259277\n",
      "train_loss:  5.6305437088012695\n",
      "train_loss:  5.656525135040283\n",
      "train_loss:  5.9667229652404785\n",
      "train_loss:  5.804852485656738\n",
      "train_loss:  5.898532867431641\n",
      "train_loss:  5.46487283706665\n",
      "train_loss:  5.602201461791992\n",
      "train_loss:  5.533041477203369\n",
      "train_loss:  5.591695785522461\n",
      "train_loss:  5.696419715881348\n",
      "train_loss:  5.502373695373535\n",
      "train_loss:  5.87254524230957\n",
      "train_loss:  5.792060375213623\n",
      "train_loss:  5.867152690887451\n",
      "train_loss:  5.760947227478027\n",
      "train_loss:  5.967029571533203\n",
      "train_loss:  5.764265060424805\n",
      "train_loss:  5.727407932281494\n",
      "train_loss:  5.680461406707764\n",
      "train_loss:  5.620405197143555\n",
      "train_loss:  5.738028526306152\n",
      "train_loss:  5.6222639083862305\n",
      "train_loss:  5.7403178215026855\n",
      "train_loss:  5.7763776779174805\n",
      "train_loss:  5.735922336578369\n",
      "train_loss:  5.642114162445068\n",
      "train_loss:  5.781014442443848\n",
      "train_loss:  5.842193603515625\n",
      "train_loss:  5.880353927612305\n",
      "train_loss:  5.559103012084961\n",
      "train_loss:  5.383917331695557\n",
      "train_loss:  5.6229166984558105\n",
      "train_loss:  5.63201904296875\n",
      "train_loss:  5.32609748840332\n",
      "train_loss:  5.2286481857299805\n",
      "train_loss:  5.674581527709961\n",
      "train_loss:  5.634850978851318\n",
      "train_loss:  5.931163311004639\n",
      "train_loss:  5.636075496673584\n",
      "train_loss:  5.692688465118408\n",
      "train_loss:  5.3921918869018555\n",
      "train_loss:  6.009560585021973\n",
      "train_loss:  5.847237586975098\n",
      "train_loss:  5.758275985717773\n",
      "train_loss:  5.61287784576416\n",
      "train_loss:  5.7722978591918945\n",
      "train_loss:  5.7781596183776855\n",
      "train_loss:  5.790025234222412\n",
      "train_loss:  5.80869197845459\n",
      "train_loss:  5.717134952545166\n",
      "train_loss:  5.925567626953125\n",
      "train_loss:  5.998994827270508\n",
      "train_loss:  5.884064197540283\n",
      "train_loss:  5.638348579406738\n",
      "train_loss:  5.4127020835876465\n",
      "train_loss:  5.710212230682373\n",
      "train_loss:  5.688075065612793\n",
      "train_loss:  5.701421737670898\n",
      "train_loss:  5.857459545135498\n",
      "train_loss:  5.7381272315979\n",
      "train_loss:  5.744265079498291\n",
      "train_loss:  5.815615177154541\n",
      "train_loss:  5.6410298347473145\n",
      "train_loss:  5.7688140869140625\n",
      "train_loss:  5.927617073059082\n",
      "train_loss:  5.840233325958252\n",
      "train_loss:  5.744781494140625\n",
      "train_loss:  5.847404956817627\n",
      "train_loss:  5.770545482635498\n",
      "train_loss:  5.794821262359619\n",
      "train_loss:  6.069771766662598\n",
      "train_loss:  5.695294380187988\n",
      "train_loss:  5.76736307144165\n",
      "train_loss:  5.925537586212158\n",
      "train_loss:  5.761320114135742\n",
      "train_loss:  5.753138542175293\n",
      "train_loss:  5.820399761199951\n",
      "train_loss:  5.849038124084473\n",
      "train_loss:  5.595994472503662\n",
      "train_loss:  5.748662948608398\n",
      "train_loss:  5.534286975860596\n",
      "Epoch 2, Train Loss: 5.784841949982937, Validation Perplexity: 265.2582092285156\n"
     ]
    }
   ],
   "source": [
    "model_rnn = train_rnn(batch_size=100, lr=0.01, n_epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance goal\n",
    "\n",
    "**Your submitted notebook must contain output demonstrating a validation perplexity of at most 280.** If you do not reach this perplexity after the first epoch, try training for a second epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Parameter initialisation (3&nbsp;points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error surfaces explored when training neural networks can be very complex. Because of this, it is important to choose ‚Äúgood‚Äù initial values for the parameters. In PyTorch, the weights of the embedding layer are initialised by sampling from the standard normal distribution $\\mathcal{N}(0, 1)$. Test how changing the initialisation affects the perplexity of your feed-forward language model. Find research articles that propose different initialisation strategies.\n",
    "\n",
    "Write a short (150&nbsp;words) report about your experiments and literature search. Use the following prompts:\n",
    "\n",
    "* What different initialisation did you try? What results did you get?\n",
    "* How do your results compare to what was suggested by the research articles?\n",
    "* What did you learn? How, exactly, did you learn it? Why does this learning matter?\n",
    "\n",
    "You are allowed to consult sources for this problem if you appropriately cite them. If in doubt, please read the [Academic Integrity Policy](https://www.ida.liu.se/~TDDE09/logistics/policies.html#academic-integrity-policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*At first we decided to try different values for the batch_size and lr parameters to find what was best suitable for this particular task. In the end we decided on a batch_size of 100 and a loss rate of 0.01. In the interest of time we only trained with one Epoch, even though it took us two Epochs to reach a perplexity better than 280. We will compare it to default settings with only one Epoch. *\n",
    "\n",
    "Two articles regarding weight initialization were considered when testing different configurations. First, the paper ‚ÄúUnderstanding the difficulty of training deep feedforward neural networks‚Äù by Xavier Glorot and Yoshua Bengio inspired us to try to initialize our weights using the ‚Äúxavier_normal‚Äù initialization that is part of the pytorch library.  The initialization gave a slight improvement in perplexity (314 vs 317) compared to the default weight initialization of pytorch.\n",
    "We also found in the paper ‚ÄúAn Exploration of Word Embedding Initialization\n",
    "in Deep-Learning Tasks‚Äù by Tom Kocmi and Ondrej Bojar that a weight initialization using a normal distribution of N(0, 0.01) instead of N(0,1) did well when they were exploring weight initializations for NLP tasks. We tried this with torch.nn.init.normal_ in pytorch. In our case we got a higher perplexity score of about 330 vs 315.\n",
    "What we can say from the experiments is that different weight initializations warrants different results and is likely another part one needs to consider when choosing your model.\n",
    "\n",
    "\n",
    "Sources:\n",
    "\n",
    "Glorot, X., & Bengio, Y. (2010, March). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256). JMLR Workshop and Conference Proceedings.\n",
    "\n",
    "Kocmi, T., & Bojar, O. (2017). An exploration of word embedding initialization in deep-learning tasks. arXiv preprint arXiv:1711.09160.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
