{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1: Language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will implement and train two neural language models: the fixed-window model and the recurrent neural network model. You will evaluate these models by computing their perplexity on a benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you should use the GPU if you have one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this lab is [WikiText](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/), a collection of more than 100 million tokens extracted from the ‚ÄúGood‚Äù and ‚ÄúFeatured‚Äù articles on Wikipedia. We will use the small version of the dataset, which contains slightly more than 2.5 million tokens.\n",
    "\n",
    "The next cell contains code for an object that will act as a container for the ‚Äútraining‚Äù and the ‚Äúvalidation‚Äù section of the data. We fill this container by reading the corresponding text files. The only processing we do is to whitespace-tokenise, and to enclose each non-empty line within `<bos>` (beginning-of-sentence) and `<eos>` (end-of-sentence) tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiText(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.train = self.read_data('wiki.train.tokens')\n",
    "        self.valid = self.read_data('wiki.valid.tokens')\n",
    "    \n",
    "    def read_data(self, path):\n",
    "        ids = []\n",
    "        with open(path, encoding='utf-8') as source:\n",
    "            for line in source:\n",
    "                line = line.rstrip()\n",
    "                if line:\n",
    "                    for token in ['<bos>'] + line.split() + ['<eos>']:\n",
    "                        if token not in self.vocab:\n",
    "                            self.vocab[token] = len(self.vocab)\n",
    "                        ids.append(self.vocab[token])\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below loads the data and prints the total number of tokens and the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in train: 2099444\n",
      "Tokens in valid: 218808\n",
      "Vocabulary size: 33279\n"
     ]
    }
   ],
   "source": [
    "wikitext = WikiText()\n",
    "print('Tokens in train:', len(wikitext.train))\n",
    "print('Tokens in valid:', len(wikitext.valid))\n",
    "print('Vocabulary size:', len(wikitext.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Fixed-window model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement and train the fixed-window neural language model proposed by [Bengio et al. (2003)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) and presented in the lectures. Recall that an input to the network takes the form of a vector of $n-1$ integers representing the preceding words. Each integer is mapped to a vector via an embedding layer. (All positions share the same embedding.) The embedding vectors are then concatenated and sent through a two-layer feed-forward network with a non-linearity in the form of a rectified linear unit (ReLU) and a final softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.1: Vectorise the data (1&nbsp;point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to write code for transforming the data in the WikiText container into a vectorised form that can be fed to the fixed-window model. Concretely, you will implement a [collate function](https://pytorch.org/docs/stable/data.html#dataloader-collate-fn) in the form of a callable vectoriser object. Complete the skeleton code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowVectorizer(object):\n",
    "    def __init__(self, n):\n",
    "        # n-gram order\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # TODO: Replace the following line with your own code\n",
    "        N = len(data) - (self.n-1) # calculate how many n-grams in the dataset \n",
    "\n",
    "        X = [data[i:i+self.n-1] for i in range(N)]\n",
    "        y = [data[i+self.n-1] for i in range(N) ]                               # [ , )\n",
    "        return torch.tensor(X), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should implement the following specification:\n",
    "\n",
    "**__init__** (*self*, *n*)\n",
    "\n",
    "> Creates a new vectoriser with n-gram order $n$. Your code should be able to handle arbitrary n-gram orders $n \\geq 1$.\n",
    "\n",
    "**__call__** (*self*, *data*)\n",
    "\n",
    "> Transforms WikiText *data* (a list of word ids) into a pair of tensors $\\mathbf{X}$, $\\mathbf{y}$ that can be used to train the fixed-window model. Let $N$ be the total number of $n$-grams from the token list; then $\\mathbf{X}$ is a matrix with shape $(N, n-1)$ and $\\mathbf{y}$ is a vector with length $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your implementation by running the code in the next cell. Does the output match your expectation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x, valid_y = FixedWindowVectorizer(2)(wikitext.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = FixedWindowVectorizer(2)(wikitext.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2: Implement the model (2&nbsp;points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to implement the fixed-window model based on the graphical specification given in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n, n_words, embedding_dim=50, hidden_dim=50):\n",
    "        # n_words -- number of words in vocabulary\n",
    "        # n       -- n-gram\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_words,embedding_dim)\n",
    "        self.fc = nn.Linear((n - 1) * embedding_dim, hidden_dim) \n",
    "        self.out = nn.Linear(hidden_dim, n_words)   \n",
    "        # TODO: Add your own code\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.view(x.size(0), -1)\n",
    "        hidden = self.fc(embeds)\n",
    "        output = self.out(hidden) # output is one-hot code for every word in vocablary\n",
    "        probs = F.softmax(output, dim=1)        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the two methods:\n",
    "\n",
    "**__init__** (*self*, *n*, *n_words*, *embedding_dim*=50, *hidden_dim*=50)\n",
    "\n",
    "> Creates a new fixed-window neural language model. The argument *n* specifies the model&rsquo;s $n$-gram order. The argument *n_words* is the number of words in the vocabulary. The arguments *embedding_dim* and *hidden_dim* specify the dimensionalities of the embedding layer and the hidden layer of the feedforward network, respectively; their default value is 50.\n",
    "\n",
    "**forward** (*self*, *x*)\n",
    "\n",
    "> Computes the network output on an input batch *x*. The shape of *x* is $(B, n-1)$, where $B$ is the batch size. The output of the forward pass is a tensor of shape $(B, V)$ where $V$ is the number of words in the vocabulary.\n",
    "\n",
    "**Hint:** The most efficient way to implement the vector concatenation in this model is to use the [`view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) method.\n",
    "\n",
    "#### ü§û Test your code\n",
    "\n",
    "Test your code by instantiating the model and feeding it a batch of examples from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2684,  0.2154,  0.1707,  ..., -0.0739,  0.2872,  0.1453],\n",
      "        [-0.5042,  0.4012,  0.1209,  ...,  0.0705,  0.1328,  0.1931],\n",
      "        [ 0.0322,  0.3868, -0.0400,  ..., -0.3120,  0.3930, -0.0578],\n",
      "        [ 0.4440, -0.4100,  0.5210,  ...,  0.0647, -0.8711, -0.1738],\n",
      "        [-0.6703,  0.4635,  0.4008,  ..., -0.2705, -0.1936,  0.2240]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_words = len(wikitext.vocab)  \n",
    "model = FixedWindowModel(2, n_words)\n",
    "\n",
    "batch_size = 5\n",
    "train_x_batch = train_x[:batch_size]\n",
    "\n",
    "output = model(train_x_batch)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3: Train the model (3&nbsp;points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your final task is to write code to train the fixed-window model using minibatch gradient descent and the cross-entropy loss function. This should be a straightforward generalisation of the training loops that you have seen so far. Complete the skeleton code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size):\n",
    "    for i in range(0, len(data) - batch_size + 1, batch_size):\n",
    "        yield data[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixed_window(n, n_epochs=2, batch_size=3200, lr=0.01):\n",
    "    n_words = len(wikitext.vocab)\n",
    "    model = FixedWindowModel(n, n_words)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    total_loss = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()  # ËÆæÁΩÆÊ®°Âûã‰∏∫ËÆ≠ÁªÉÊ®°Âºè\n",
    "        for batch_x, batch_y in zip(get_batches(train_x, batch_size), get_batches(train_y, batch_size)):\n",
    "            # batch_x Âíå batch_y ÊòØÈÖçÂØπÁöÑËæìÂÖ•ÂíåÁõÆÊ†á\n",
    "\n",
    "            # Ê∏ÖÁ©∫Ê¢ØÂ∫¶\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # ÂâçÂêë‰º†Êí≠\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # ËÆ°ÁÆóÊçüÂ§±\n",
    "            loss = F.cross_entropy(outputs, batch_y)\n",
    "            print(\"train_loss: \",loss)\n",
    "            # ÂèçÂêë‰º†Êí≠Âíå‰ºòÂåñ\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    train_loss = total_loss/len(train_x)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x,batch_y in zip(get_batches(valid_x, batch_size), get_batches(valid_y, batch_size)):\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.long)\n",
    "            outputs = model(batch_x)\n",
    "            total_loss += F.cross_entropy(outputs, batch_y, reduction='sum').item()\n",
    "\n",
    "        # ËÆ°ÁÆóÈ™åËØÅÈõÜ‰∏äÁöÑÂπ≥ÂùáÊçüÂ§±\n",
    "    valid_loss = total_loss / len(valid_x)\n",
    "    perplexity = torch.exp(torch.tensor(valid_loss))\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Validation Perplexity: {perplexity}') \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the training function:\n",
    "\n",
    "**train_fixed_window** (*n*, *n_epochs* = 1, *batch_size* = 3200, *lr* = 0.01)\n",
    "\n",
    "> Trains a fixed-window neural language model of order *n* using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*. After each epoch, prints the perplexity of the model on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below trains a bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  tensor(10.4926, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(10.3766, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(10.2626, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(10.1719, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(10.0996, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(9.8421, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(9.8001, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(9.5943, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(9.3518, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(9.2322, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(9.0240, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(8.9626, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(8.9888, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(8.8243, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(8.6283, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(8.3033, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(8.3118, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(8.2905, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.9864, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(8.0292, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(8.0201, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.9559, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.9926, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.8298, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.5502, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.5066, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.3093, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.4645, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.1662, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.3378, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.2343, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0862, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0565, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0091, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0094, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.1080, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0557, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0486, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0163, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9370, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7954, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.2282, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9715, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0471, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0139, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0297, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8689, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7303, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0919, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0465, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0500, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5829, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8357, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7644, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8410, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9161, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8688, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6886, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7210, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0230, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9675, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5397, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7551, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6381, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5640, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4571, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6034, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.1285, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.1321, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6834, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9495, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9220, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9700, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7784, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8528, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7374, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8428, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8133, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8365, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8746, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7408, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0259, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8634, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6155, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7544, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5962, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5507, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7798, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7569, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6532, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8904, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7354, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9023, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0683, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0134, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8998, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6957, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7264, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9276, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9122, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5621, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6701, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0354, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8936, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8262, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7176, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9988, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6484, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8492, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5486, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9026, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7830, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8119, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8115, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6571, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7611, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6581, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8158, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8334, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5373, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6719, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5593, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5203, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6332, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9077, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6658, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7431, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5007, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4115, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2805, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2776, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3608, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5173, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6101, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5561, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5289, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5454, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4858, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6754, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6481, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6724, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6199, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5736, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9277, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5449, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3557, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5077, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7770, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6739, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4728, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3761, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5860, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7303, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6685, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4548, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1686, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4348, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6456, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4591, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4579, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5840, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4459, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3472, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4822, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6535, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7242, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5098, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2979, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2173, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8584, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(7.0808, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6587, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5342, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3803, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5297, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5901, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4705, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5988, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6942, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6414, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6591, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4650, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5655, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5250, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5383, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1175, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1863, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3780, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7331, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2679, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6799, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6412, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4183, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1851, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5251, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3733, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3420, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3052, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7136, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5649, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5222, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2940, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5539, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4883, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6181, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7007, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4397, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5006, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3145, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3097, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3872, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4918, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3605, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5154, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.9865, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7185, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7623, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7232, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5848, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5947, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2847, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7116, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4553, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5049, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4542, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5409, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2977, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4748, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4309, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4890, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1462, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4125, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2445, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2835, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9688, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3489, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5809, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4730, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3753, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5139, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4977, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3019, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4318, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2847, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3477, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5497, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4203, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3821, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4263, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1978, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2708, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0473, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0399, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4680, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3150, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7213, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5146, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5267, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6280, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2840, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5748, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4956, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4625, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5203, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1630, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1928, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8519, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9980, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0859, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5434, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7745, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5655, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5929, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7462, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5094, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5151, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5309, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0323, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1037, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4726, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5601, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4756, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4490, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4902, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3935, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2979, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3842, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4549, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3096, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3095, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3389, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0292, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1630, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3503, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5951, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5437, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4559, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5224, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5519, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5210, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3040, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5948, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5690, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3569, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4765, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1211, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1791, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1631, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1450, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2531, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7102, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5652, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5844, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4814, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1576, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6679, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2182, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2859, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4180, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2865, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2374, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0494, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1814, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1439, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1030, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1323, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5514, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5160, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1316, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2498, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1125, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3301, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3079, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2251, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0450, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2475, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1916, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2627, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4726, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2686, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0643, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7945, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7073, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3873, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4152, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2173, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4902, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5418, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4755, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4511, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2727, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5544, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4388, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7642, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1723, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2633, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3467, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2190, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3234, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2716, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1708, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2149, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1880, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3908, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5148, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1429, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9789, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1083, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2579, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6005, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1317, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2758, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2261, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2359, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3915, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3500, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2251, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2340, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2208, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0870, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9623, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2890, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1179, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0272, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0272, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0777, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4639, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2221, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1123, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3028, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1190, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2434, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4656, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4498, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4593, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1234, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4227, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2043, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2923, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5184, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4560, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1856, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2639, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2044, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4135, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8202, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8704, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5351, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4198, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1672, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9698, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2452, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3041, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2480, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3454, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3463, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1870, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3245, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1215, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3198, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3720, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4241, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1835, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3341, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9950, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3975, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3637, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3778, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1266, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0185, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0656, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4769, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4821, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3691, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2718, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2151, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2872, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8793, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1360, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4149, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4756, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3360, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3752, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3988, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4497, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0921, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1047, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.7312, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3917, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3194, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3973, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1316, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4957, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2828, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1159, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1506, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9539, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0277, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1406, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2973, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3343, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4839, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3429, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3460, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1289, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6455, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7165, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2872, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2379, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2345, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2960, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3218, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5130, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2786, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2165, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8692, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3715, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0350, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3964, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0194, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1495, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3935, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2653, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1937, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1885, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2687, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1602, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2834, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3839, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1242, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9911, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3610, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3451, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0686, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1464, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2443, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0710, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6905, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5135, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1402, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0254, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0199, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9359, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0208, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3171, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3050, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1911, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8800, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3415, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1197, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1428, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1634, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6644, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0987, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1460, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.8382, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3170, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2175, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3106, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1371, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4866, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0387, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0843, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0664, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1465, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1187, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1524, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9923, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2202, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5561, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4531, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4554, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4724, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2092, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1430, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0783, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2708, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4101, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2061, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1844, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1447, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3568, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2286, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4994, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6677, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5564, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4312, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2481, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9878, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1193, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0714, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4671, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4478, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3115, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2563, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3765, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1015, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2508, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4148, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0604, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1674, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5386, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4521, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4309, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9283, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0234, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0955, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0790, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1046, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2729, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2107, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0623, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2191, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1753, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4748, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1818, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1939, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5266, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3891, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4401, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0977, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1989, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0170, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1157, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1740, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9994, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3591, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3252, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3311, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2022, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5092, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2281, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2640, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2392, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1725, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2118, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0863, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2374, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2960, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1426, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0401, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3224, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3673, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3598, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1047, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9450, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1886, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0880, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7750, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6757, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0701, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9869, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3082, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0326, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2590, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9948, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.6590, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4385, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3687, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3854, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2959, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2325, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2648, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3711, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2775, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4254, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4699, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3593, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1155, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8426, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2909, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1882, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1901, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3030, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2033, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2018, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3132, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1303, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3239, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5077, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3270, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1846, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2620, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2492, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2888, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.5757, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0950, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1747, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.4263, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2812, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2632, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2669, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2831, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0991, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2003, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9895, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0836, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8889, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0672, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2127, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0737, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7661, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8673, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0226, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9181, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9017, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9269, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0343, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0936, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2139, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1696, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9189, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9297, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9673, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9175, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7888, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9356, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8911, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0355, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9370, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7786, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9258, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9350, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0107, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7624, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9160, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9494, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8111, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8348, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8180, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8195, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9669, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8740, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8501, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8693, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8253, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6836, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9724, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7842, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9872, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6419, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9505, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8548, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6290, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9613, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0166, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0606, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5734, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7838, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8734, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8111, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8340, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8141, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6046, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6461, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9049, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9536, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5934, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8247, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7503, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7083, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5256, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5831, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1595, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0841, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7430, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9542, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8712, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9723, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8261, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9583, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7779, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8915, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8989, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7932, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9832, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8025, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1504, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9601, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7540, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9442, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9053, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8482, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0397, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9211, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8411, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9629, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8983, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0911, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1307, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1207, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9982, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8687, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7292, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8510, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9476, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7684, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8799, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2852, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0283, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0522, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0179, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.3455, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8880, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9424, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7023, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0767, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0186, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1199, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8669, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9096, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9664, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9669, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9766, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9716, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8296, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9177, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8236, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6942, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9558, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1700, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9018, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0659, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8475, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7625, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6071, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5868, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6877, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7576, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8555, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7974, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8624, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9319, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6998, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8411, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9260, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0012, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9325, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8614, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1203, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8385, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6851, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8435, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9628, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9667, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7653, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6578, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8058, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9303, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9482, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8166, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4727, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7842, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8789, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4333, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8271, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8683, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7298, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6926, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6670, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9874, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0482, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8455, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6653, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6357, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0809, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2983, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8745, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8231, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7064, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8130, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8444, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7615, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9677, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0519, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9069, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9260, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7820, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8988, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8886, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9220, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5142, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5785, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7017, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0724, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5939, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9042, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0190, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8299, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5624, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9480, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7205, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4640, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6034, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1450, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0202, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0549, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8287, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9293, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8922, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0731, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1379, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9024, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9282, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8450, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6537, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7859, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8579, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7614, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9128, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2863, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9034, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9057, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0636, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7099, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0140, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6927, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9950, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8746, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8269, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7763, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9517, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7323, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9185, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8750, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8636, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5792, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7942, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6353, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6773, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4584, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7220, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9674, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8396, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8759, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0053, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9129, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7127, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8244, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6966, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8147, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0748, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9400, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6937, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8663, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7171, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8385, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6308, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5451, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8613, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6206, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0102, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9152, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9510, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0696, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6861, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0239, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8806, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8916, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9937, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6142, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7048, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4442, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4743, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5652, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9484, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9569, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8248, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8440, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1421, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0077, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9260, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7774, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5482, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7004, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9775, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9902, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9650, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9017, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9534, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8013, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6984, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7963, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8174, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7201, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7488, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7991, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5703, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6099, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8422, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9851, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9372, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9117, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9585, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9908, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9474, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6539, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8324, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8151, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6728, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9413, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5202, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6098, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6992, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6038, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6679, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9188, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8019, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6896, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5675, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.3611, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1246, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7914, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8119, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0252, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8970, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9032, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6519, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7582, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7736, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6713, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6711, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1837, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0306, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7112, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7416, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5992, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8552, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8564, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7700, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5284, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7606, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8260, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8389, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9392, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8319, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6527, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2492, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1529, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8388, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9230, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6772, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9752, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0852, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0251, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9539, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7407, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9701, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8760, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1869, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8057, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8990, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9055, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7784, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9900, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8603, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7576, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7930, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7858, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8977, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0773, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7307, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5666, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7228, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8996, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0391, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6642, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8446, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8155, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8052, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9031, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8544, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8176, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8056, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7904, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6702, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5802, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8065, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6436, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5562, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4315, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5214, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1007, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8415, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6812, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8155, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6444, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7462, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9121, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9752, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9755, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6465, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9132, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7229, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8503, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1053, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0139, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7326, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8380, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7477, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8059, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.3379, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.3984, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0937, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9664, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7906, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6384, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8478, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8883, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8695, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8654, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9267, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7880, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8800, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6593, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8025, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9235, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9170, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7784, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9931, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5532, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8947, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8490, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9104, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7539, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5610, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6231, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9692, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9327, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6965, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8122, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7971, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9017, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2919, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6848, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0609, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1090, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9794, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9138, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9295, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9507, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7431, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6261, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0286, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8672, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7112, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0126, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7150, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0490, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8642, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7723, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7148, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6289, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6144, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7241, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8511, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9285, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9381, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8908, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9111, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6106, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.0881, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.2321, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9322, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8861, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9265, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8906, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9774, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1185, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8900, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8574, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5150, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9447, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7298, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8837, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6635, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8392, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9088, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7553, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8155, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7754, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8294, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7667, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8381, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0325, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7508, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6184, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0461, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9542, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7651, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7570, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6927, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6257, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1426, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1149, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7691, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6494, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6345, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5830, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6545, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9594, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9527, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7791, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4731, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9242, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7347, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7747, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7840, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1320, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7059, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8327, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1369, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7813, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8940, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9609, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7221, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0456, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6365, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6991, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7390, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7993, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7578, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8245, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6308, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8197, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0538, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9558, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9308, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9017, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7120, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7367, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7133, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9104, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0814, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8683, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8681, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8382, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0701, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9002, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9802, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2822, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1774, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0787, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8116, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5730, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7798, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7059, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9446, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0373, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8581, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7735, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8799, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6478, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8512, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9714, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7387, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8314, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1371, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0446, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0743, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.4598, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6208, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6701, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6492, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7085, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8641, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8622, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7116, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9011, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8334, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1276, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8111, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8563, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1050, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9619, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0366, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6984, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7686, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6470, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7194, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8073, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5736, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9954, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8485, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0075, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8809, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0449, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8772, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8507, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7387, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7124, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8423, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7987, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8692, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9059, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8306, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6348, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8670, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9452, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9535, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6609, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5490, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7188, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7411, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.5013, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.3528, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8114, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7288, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0578, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7163, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8782, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6096, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1977, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0048, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8427, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6524, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8802, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9706, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9728, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9646, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9009, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1119, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.1311, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0308, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7624, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6445, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7965, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7437, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8258, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0148, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8776, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8773, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9343, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7628, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9425, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0346, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0014, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8915, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9615, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9784, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8956, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.2086, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7548, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.7927, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(6.0585, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8695, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8699, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9255, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.9063, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8358, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.8866, grad_fn=<NllLossBackward0>)\n",
      "train_loss:  tensor(5.6762, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17456\\AppData\\Local\\Temp\\ipykernel_55960\\87636936.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_y = torch.tensor(batch_y, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.0038611886944710174, Validation Perplexity: 344.9189147949219\n"
     ]
    }
   ],
   "source": [
    "model_fixed_window = train_fixed_window(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance goal\n",
    "\n",
    "**Your submitted notebook must contain output demonstrating a validation perplexity of at most 350.** If you do not reach this perplexity after the first epoch, try training for a second epoch.\n",
    "\n",
    "‚ö†Ô∏è Computing the validation perplexity in one go (for the full validation set) will probably exhaust your computer‚Äôs memory and/or take a lot of time. If you run into this problem, do the computation at the minibatch level and aggregate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "To see whether your network is learning something, print or plot the loss and/or the perplexity on the training data. If the two values do not decrease during training, try to find the problem before wasting time (and electricity) on useless computation.\n",
    "\n",
    "Training and even evaluation will take some time ‚Äì on a CPU, you should expect several minutes per epoch, depending on hardware. Our reference implementation uses a GPU and runs in less than 30 seconds per epoch on [Colab](http://colab.research.google.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Recurrent neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement the recurrent neural network language model. Recall that an input to this model is a vector of word ids. Each integer is mapped to an embedding vector. The sequence of embedded vectors is then fed into an unrolled LSTM. At each position $i$ in the sequence, the hidden state of the LSTM at that position is sent through a linear transformation into a final softmax layer representing the probability distribution over the words at position $i+1$. In theory, the input vector could represent the complete training data; for practical reasons, however, we will truncate the input to some fixed value *bptt_len*. This length is called the **backpropagation-through-time horizon**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1: Vectorise the data (1&nbsp;point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous problem, your first task is to transform the data in the WikiText container into a vectorised form that can be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNVectorizer(object):\n",
    "    def __init__(self, bptt_len):\n",
    "        # backpropagation-through-time horizon\n",
    "        self.bptt_len = bptt_len\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # TODO: Replace the following line with your own code\n",
    "        num_sequences = len(data) // self.bptt_len\n",
    "\n",
    "        # Initialize X and Y tensors\n",
    "        X = torch.zeros((num_sequences, self.bptt_len), dtype=torch.long)\n",
    "        Y = torch.zeros((num_sequences, self.bptt_len), dtype=torch.long)\n",
    "\n",
    "        for i in range(num_sequences):\n",
    "            start_idx = i * self.bptt_len\n",
    "            end_idx = start_idx + self.bptt_len\n",
    "\n",
    "            # Populate X with the current sequence\n",
    "            X[i] = torch.tensor(data[start_idx:end_idx])\n",
    "\n",
    "            # Populate Y with the sequence offset by one position\n",
    "            Y[i] = torch.tensor(data[start_idx + 1:end_idx + 1] if end_idx < len(data) else data[start_idx + 1:] + [0])\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your vectoriser should meet the following specification:\n",
    "\n",
    "**__init__** (*self*, *bptt_len*)\n",
    "\n",
    "> Creates a new vectoriser. The parameter *bptt_len* specifies the backpropagation-through-time horizon.\n",
    "\n",
    "**__call__** (*self*, *data*)\n",
    "\n",
    "> Transforms a list of token indexes *data* into a pair of tensors $\\mathbf{X}$, $\\mathbf{Y}$ that can be used to train the recurrent neural language model. The rows of both tensors represent contiguous subsequences of token indexes of length *bptt_len*. Compared to the sequences in $\\mathbf{X}$, the corresponding sequences in $\\mathbf{Y}$ are shifted one position to the right. More precisely, if the $i$th row of $\\mathbf{X}$ is the sequence that starts at token position $j$, then the same row of $\\mathbf{Y}$ is the sequence that starts at position $j+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your implementation by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6837, 32]) torch.Size([6837, 32])\n"
     ]
    }
   ],
   "source": [
    "valid_x, valid_y = RNNVectorizer(32)(wikitext.valid)\n",
    "\n",
    "print(valid_x.size(), valid_y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = RNNVectorizer(32)(wikitext.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  1,  5,  0,  6,  7,  2,  8,  9, 10,  3, 11, 12,  9,\n",
       "        13, 14, 15, 16,  2, 17, 18, 19,  8, 20, 14, 21, 22, 23])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4,  1,  5,  0,  6,  7,  2,  8,  9, 10,  3, 11, 12,  9, 13,\n",
      "        14, 15, 16,  2, 17, 18, 19,  8, 20, 14, 21, 22, 23, 24])\n"
     ]
    }
   ],
   "source": [
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2: Implement the model (2&nbsp;points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to implement the recurrent neural network model based on the graphical specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_words, embedding_dim=50, hidden_dim=50):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_words, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim, n_words)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.LSTM(x)\n",
    "        out = self.fc(lstm_out)\n",
    "        #out = F.softmax(out, dim = -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should follow this specification:\n",
    "\n",
    "**__init__** (*self*, *n_words*, *embedding_dim* = 50, *hidden_dim* = 50)\n",
    "\n",
    "> Creates a new recurrent neural network language model based on an LSTM. The argument *n_words* is the number of words in the vocabulary. The arguments *embedding_dim* and *hidden_dim* specify the dimensionalities of the embedding layer and the LSTM hidden layer, respectively; their default value is 50.\n",
    "\n",
    "**forward** (*self*, *x*)\n",
    "\n",
    "> Computes the network output on an input batch *x*. The shape of *x* is $(B, H)$, where $B$ is the batch size and $H$ is the length of each input sequence. The shape of the output tensor is $(B, H, V)$, where $V$ is the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your code by instantiating the model and feeding it a batch of examples from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,     1, 32967, 32968,     1,     5,     0, 32967, 32968,    14,\n",
      "          407,    24,    18,  6254, 19903,   311,  1445, 19903,    14,    27,\n",
      "           28,  2577,    17,    10, 19903,   116,    18,  4930,  4122,  9612,\n",
      "           14,  4855])\n",
      "tensor([[ 0.0219, -0.0266, -0.0625,  ...,  0.0937,  0.1366, -0.1284],\n",
      "        [-0.0493, -0.1852, -0.1740,  ...,  0.1004,  0.2059, -0.0193],\n",
      "        [-0.0632, -0.0537,  0.0171,  ..., -0.0079,  0.0997, -0.0614],\n",
      "        ...,\n",
      "        [-0.1008,  0.0491, -0.1663,  ..., -0.0189,  0.1513, -0.1380],\n",
      "        [ 0.0257,  0.0202, -0.0204,  ..., -0.0980,  0.0571, -0.1446],\n",
      "        [ 0.0385,  0.0463, -0.1642,  ..., -0.0316,  0.0994,  0.0009]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(valid_x[0])\n",
    "model = RNNModel(n_words=len(wikitext.vocab))\n",
    "output = model(valid_x[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3: Train the model (3&nbsp;points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop for the recurrent neural network model is essentially identical to the loop that you wrote for the feed-forward model. The only thing to note is that the cross-entropy loss function expects its input to be a two-dimensional tensor; you will therefore have to re-shape the output tensor from the LSTM as well as the gold-standard output tensor in a suitable way. The most efficient way to do so is to use the [`view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ...,    21,    22,    23],\n",
       "        [   24,     2,     3,  ...,    44,    26,    14],\n",
       "        [   47,    27,    18,  ...,    23,    18,    60],\n",
       "        ...,\n",
       "        [  349,     7,   428,  ...,   285, 23961,    27],\n",
       "        [  495,   490,   152,  ...,  4855,  2491,    16],\n",
       "        [   84,  9617,    27,  ...,   565,  1363,   152]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size):\n",
    "    for i in range(0, len(data) - batch_size + 1, batch_size):\n",
    "        yield data[i:i + batch_size]\n",
    "\n",
    "def train_rnn(n_epochs=1, batch_size=1000, lr=0.01):\n",
    "    # Initialize the model\n",
    "    model = RNNModel(n_words=len(wikitext.vocab))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Iterate over batches\n",
    "        for batch_x, batch_y in zip(get_batches(train_x, batch_size), get_batches(train_y, batch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "\n",
    "            # Reshape outputs and targets to fit CrossEntropyLoss expectations\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            batch_y = batch_y.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, batch_y)# why is better than F.cross_entropy\n",
    "            print(\"train_loss: \", loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss = total_loss/len(train_x)\n",
    "\n",
    "        # Calculate perplexity on validation data\n",
    "        # perplexity = calculate_perplexity(model, validation_data, batch_size, ...)\n",
    "        # print(f'Epoch {epoch+1}, Perplexity: {perplexity}')\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in zip(get_batches(valid_x, batch_size), get_batches(valid_y, batch_size)):\n",
    "                batch_y = torch.tensor(batch_y, dtype=torch.long)\n",
    "                outputs = model(batch_x)\n",
    "\n",
    "                outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                batch_y = batch_y.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, batch_y)# why is better than F.cross_entropy\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        # ËÆ°ÁÆóÈ™åËØÅÈõÜ‰∏äÁöÑÂπ≥ÂùáÊçüÂ§±\n",
    "    valid_loss = total_loss / len(valid_x)\n",
    "    perplexity = torch.exp(torch.tensor(valid_loss))\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Validation Perplexity: {perplexity}') \n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Additional functions like 'create_batches' and 'calculate_perplexity' need to be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the training function:\n",
    "\n",
    "**train_rnn** (*n_epochs* = 1, *batch_size* = 1000, *bptt_len* = 32, *lr* = 0.01)\n",
    "\n",
    "> Trains a recurrent neural network language model on the WikiText data using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. The parameter *bptt_len* specifies the length of the backpropagation-through-time horizon, that is, the length of the input and output sequences. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*. After each epoch, prints the perplexity of the model on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model by running the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  10.433974266052246\n",
      "train_loss:  10.363835334777832\n",
      "train_loss:  10.27663803100586\n",
      "train_loss:  10.114789009094238\n",
      "train_loss:  9.857285499572754\n",
      "train_loss:  9.405230522155762\n",
      "train_loss:  8.930143356323242\n",
      "train_loss:  8.54808235168457\n",
      "train_loss:  8.156769752502441\n",
      "train_loss:  7.964465618133545\n",
      "train_loss:  7.716857433319092\n",
      "train_loss:  7.578598499298096\n",
      "train_loss:  7.436985015869141\n",
      "train_loss:  7.399197101593018\n",
      "train_loss:  7.502132415771484\n",
      "train_loss:  7.4681572914123535\n",
      "train_loss:  7.534809112548828\n"
     ]
    }
   ],
   "source": [
    "model_rnn = train_rnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance goal\n",
    "\n",
    "**Your submitted notebook must contain output demonstrating a validation perplexity of at most 280.** If you do not reach this perplexity after the first epoch, try training for a second epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Parameter initialisation (3&nbsp;points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error surfaces explored when training neural networks can be very complex. Because of this, it is important to choose ‚Äúgood‚Äù initial values for the parameters. In PyTorch, the weights of the embedding layer are initialised by sampling from the standard normal distribution $\\mathcal{N}(0, 1)$. Test how changing the initialisation affects the perplexity of your feed-forward language model. Find research articles that propose different initialisation strategies.\n",
    "\n",
    "Write a short (150&nbsp;words) report about your experiments and literature search. Use the following prompts:\n",
    "\n",
    "* What different initialisation did you try? What results did you get?\n",
    "* How do your results compare to what was suggested by the research articles?\n",
    "* What did you learn? How, exactly, did you learn it? Why does this learning matter?\n",
    "\n",
    "You are allowed to consult sources for this problem if you appropriately cite them. If in doubt, please read the [Academic Integrity Policy](https://www.ida.liu.se/~TDDE09/logistics/policies.html#academic-integrity-policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Enter your text here*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
